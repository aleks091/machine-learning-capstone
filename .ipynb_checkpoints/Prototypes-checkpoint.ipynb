{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "import scipy\n",
    "from scipy import misc\n",
    "import os\n",
    "\n",
    "# load the data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = np.squeeze(y_train)\n",
    "print('data loaded')\n",
    "\n",
    "# load inceptionV3 model + remove final classification layers\n",
    "model = InceptionV3(weights='imagenet', include_top=False, input_shape=(139, 139, 3))\n",
    "print('model loaded')\n",
    "\n",
    "# obtain bottleneck features (train)\n",
    "if os.path.exists('inception_features_train.npz'):\n",
    "    print('bottleneck features detected (train)')\n",
    "    features = np.load('inception_features_train.npz')['features']\n",
    "else:\n",
    "    print('bottleneck features file not detected (train)')\n",
    "    print('calculating now ...')\n",
    "    # pre-process the train data\n",
    "    big_x_train = np.array([scipy.misc.imresize(x_train[i], (139, 139, 3)) \n",
    "                            for i in range(0, len(x_train))]).astype('float32')\n",
    "    inception_input_train = preprocess_input(big_x_train)\n",
    "    print('train data preprocessed')\n",
    "    # extract, process, and save bottleneck features\n",
    "    features = model.predict(inception_input_train)\n",
    "    features = np.squeeze(features)\n",
    "    np.savez('inception_features_train', features=features)\n",
    "print('bottleneck features saved (train)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 1: Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 total oct categories.\n",
      "There are 4562 total oct images.\n",
      "\n",
      "There are 2624 training oct images.\n",
      "There are 938 validation oct images.\n",
      "There are 1000 test oct images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    oct_files = np.array(data['filenames'])\n",
    "    oct_targets = np_utils.to_categorical(np.array(data['target']), 4)\n",
    "    return oct_files, oct_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('OCT2017-REDUCED/train')\n",
    "valid_files, valid_targets = load_dataset('OCT2017-REDUCED/valid')\n",
    "test_files, test_targets = load_dataset('OCT2017-REDUCED/test')\n",
    "\n",
    "# load list of oct names\n",
    "oct_names = [item[20:-1] for item in sorted(glob(\"OCT2017-REDUCED/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total oct categories.' % len(oct_names))\n",
    "print('There are %s total oct images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training oct images.' % len(train_files))\n",
    "print('There are %d validation oct images.' % len(valid_files))\n",
    "print('There are %d test oct images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2624/2624 [00:47<00:00, 54.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:18<00:00, 51.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:21<00:00, 46.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step0'></a>\n",
    "## Step N: Create benchmark model\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 224, 224, 75)      975       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 112, 112, 75)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 112, 112, 100)     30100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 56, 56, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 56, 56, 125)       50125     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 28, 28, 125)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 28, 28, 125)       62625     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 14, 14, 125)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 14, 14, 125)       62625     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 7, 7, 125)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 7, 7, 125)         62625     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 3, 3, 125)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 3, 3, 125)         62625     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 1, 1, 125)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1, 1, 125)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 504       \n",
      "=================================================================\n",
      "Total params: 332,204\n",
      "Trainable params: 332,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model = Sequential([\n",
    "    \n",
    "    #Locally connected layer containing fewer weights\n",
    "    #Break the image up into smaller pieces\n",
    "    #Use 75 filters to identify the most general patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=75, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3)),\n",
    "    \n",
    "    #Reduce dimensionality of convolutional layer,\n",
    "    #Reduce by taking the maximum value in the filter\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    #Use 100 filters to identify the more specific patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=100, kernel_size=2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    #Use 125 filters to identify the more specific patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),\n",
    "    \n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    \n",
    "    \n",
    "    Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    MaxPooling2D(pool_size=2),\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    # Add a softmax activation layer\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2624 samples, validate on 938 samples\n",
      "Epoch 1/8\n",
      "2624/2624 [==============================] - ETA: 2:31 - loss: 1.3907 - acc: 0.150 - ETA: 1:40 - loss: 1.3904 - acc: 0.250 - ETA: 1:23 - loss: 1.4083 - acc: 0.250 - ETA: 1:15 - loss: 1.4039 - acc: 0.225 - ETA: 1:09 - loss: 1.4010 - acc: 0.240 - ETA: 1:06 - loss: 1.3978 - acc: 0.258 - ETA: 1:03 - loss: 1.3961 - acc: 0.264 - ETA: 1:01 - loss: 1.3940 - acc: 0.268 - ETA: 59s - loss: 1.3930 - acc: 0.266 - ETA: 57s - loss: 1.3941 - acc: 0.26 - ETA: 56s - loss: 1.3936 - acc: 0.26 - ETA: 55s - loss: 1.3926 - acc: 0.27 - ETA: 54s - loss: 1.3911 - acc: 0.28 - ETA: 53s - loss: 1.3925 - acc: 0.27 - ETA: 52s - loss: 1.3920 - acc: 0.28 - ETA: 51s - loss: 1.3913 - acc: 0.28 - ETA: 50s - loss: 1.3908 - acc: 0.28 - ETA: 50s - loss: 1.3901 - acc: 0.28 - ETA: 49s - loss: 1.3889 - acc: 0.27 - ETA: 48s - loss: 1.3873 - acc: 0.27 - ETA: 48s - loss: 1.3894 - acc: 0.27 - ETA: 47s - loss: 1.3887 - acc: 0.27 - ETA: 46s - loss: 1.3879 - acc: 0.27 - ETA: 46s - loss: 1.3895 - acc: 0.27 - ETA: 45s - loss: 1.3887 - acc: 0.27 - ETA: 45s - loss: 1.3896 - acc: 0.27 - ETA: 44s - loss: 1.3899 - acc: 0.26 - ETA: 44s - loss: 1.3896 - acc: 0.26 - ETA: 43s - loss: 1.3897 - acc: 0.26 - ETA: 43s - loss: 1.3895 - acc: 0.26 - ETA: 42s - loss: 1.3894 - acc: 0.26 - ETA: 42s - loss: 1.3897 - acc: 0.26 - ETA: 41s - loss: 1.3896 - acc: 0.25 - ETA: 41s - loss: 1.3893 - acc: 0.26 - ETA: 40s - loss: 1.3890 - acc: 0.26 - ETA: 40s - loss: 1.3891 - acc: 0.25 - ETA: 39s - loss: 1.3889 - acc: 0.26 - ETA: 39s - loss: 1.3885 - acc: 0.26 - ETA: 38s - loss: 1.3893 - acc: 0.26 - ETA: 38s - loss: 1.3888 - acc: 0.26 - ETA: 37s - loss: 1.3890 - acc: 0.26 - ETA: 37s - loss: 1.3888 - acc: 0.26 - ETA: 36s - loss: 1.3881 - acc: 0.26 - ETA: 36s - loss: 1.3886 - acc: 0.26 - ETA: 35s - loss: 1.3887 - acc: 0.26 - ETA: 35s - loss: 1.3887 - acc: 0.26 - ETA: 35s - loss: 1.3883 - acc: 0.26 - ETA: 34s - loss: 1.3880 - acc: 0.26 - ETA: 34s - loss: 1.3874 - acc: 0.26 - ETA: 33s - loss: 1.3876 - acc: 0.26 - ETA: 33s - loss: 1.3868 - acc: 0.26 - ETA: 32s - loss: 1.3884 - acc: 0.26 - ETA: 32s - loss: 1.3882 - acc: 0.26 - ETA: 31s - loss: 1.3879 - acc: 0.26 - ETA: 31s - loss: 1.3880 - acc: 0.26 - ETA: 31s - loss: 1.3879 - acc: 0.26 - ETA: 30s - loss: 1.3872 - acc: 0.26 - ETA: 30s - loss: 1.3880 - acc: 0.26 - ETA: 29s - loss: 1.3880 - acc: 0.26 - ETA: 29s - loss: 1.3873 - acc: 0.26 - ETA: 28s - loss: 1.3865 - acc: 0.26 - ETA: 28s - loss: 1.3862 - acc: 0.26 - ETA: 28s - loss: 1.3856 - acc: 0.26 - ETA: 27s - loss: 1.3848 - acc: 0.26 - ETA: 27s - loss: 1.3832 - acc: 0.26 - ETA: 26s - loss: 1.3830 - acc: 0.26 - ETA: 26s - loss: 1.3823 - acc: 0.26 - ETA: 25s - loss: 1.3812 - acc: 0.26 - ETA: 25s - loss: 1.3803 - acc: 0.26 - ETA: 25s - loss: 1.3796 - acc: 0.26 - ETA: 24s - loss: 1.3795 - acc: 0.26 - ETA: 24s - loss: 1.3795 - acc: 0.26 - ETA: 23s - loss: 1.3792 - acc: 0.26 - ETA: 23s - loss: 1.3788 - acc: 0.27 - ETA: 23s - loss: 1.3772 - acc: 0.27 - ETA: 22s - loss: 1.3803 - acc: 0.27 - ETA: 22s - loss: 1.3802 - acc: 0.27 - ETA: 21s - loss: 1.3798 - acc: 0.27 - ETA: 21s - loss: 1.3783 - acc: 0.27 - ETA: 20s - loss: 1.3772 - acc: 0.27 - ETA: 20s - loss: 1.3762 - acc: 0.27 - ETA: 20s - loss: 1.3734 - acc: 0.27 - ETA: 19s - loss: 1.3717 - acc: 0.28 - ETA: 19s - loss: 1.3705 - acc: 0.28 - ETA: 18s - loss: 1.3702 - acc: 0.28 - ETA: 18s - loss: 1.3697 - acc: 0.28 - ETA: 18s - loss: 1.3684 - acc: 0.28 - ETA: 17s - loss: 1.3653 - acc: 0.28 - ETA: 17s - loss: 1.3621 - acc: 0.29 - ETA: 16s - loss: 1.3589 - acc: 0.29 - ETA: 16s - loss: 1.3562 - acc: 0.29 - ETA: 15s - loss: 1.3559 - acc: 0.29 - ETA: 15s - loss: 1.3556 - acc: 0.29 - ETA: 15s - loss: 1.3542 - acc: 0.29 - ETA: 14s - loss: 1.3542 - acc: 0.29 - ETA: 14s - loss: 1.3516 - acc: 0.29 - ETA: 13s - loss: 1.3497 - acc: 0.30 - ETA: 13s - loss: 1.3485 - acc: 0.30 - ETA: 13s - loss: 1.3478 - acc: 0.30 - ETA: 12s - loss: 1.3462 - acc: 0.30 - ETA: 12s - loss: 1.3447 - acc: 0.30 - ETA: 11s - loss: 1.3438 - acc: 0.30 - ETA: 11s - loss: 1.3413 - acc: 0.31 - ETA: 11s - loss: 1.3402 - acc: 0.31 - ETA: 10s - loss: 1.3399 - acc: 0.31 - ETA: 10s - loss: 1.3386 - acc: 0.31 - ETA: 9s - loss: 1.3383 - acc: 0.3150 - ETA: 9s - loss: 1.3371 - acc: 0.316 - ETA: 9s - loss: 1.3367 - acc: 0.316 - ETA: 8s - loss: 1.3363 - acc: 0.315 - ETA: 8s - loss: 1.3356 - acc: 0.317 - ETA: 7s - loss: 1.3332 - acc: 0.318 - ETA: 7s - loss: 1.3304 - acc: 0.321 - ETA: 6s - loss: 1.3288 - acc: 0.324 - ETA: 6s - loss: 1.3288 - acc: 0.323 - ETA: 6s - loss: 1.3278 - acc: 0.325 - ETA: 5s - loss: 1.3267 - acc: 0.328 - ETA: 5s - loss: 1.3249 - acc: 0.330 - ETA: 4s - loss: 1.3226 - acc: 0.331 - ETA: 4s - loss: 1.3262 - acc: 0.330 - ETA: 4s - loss: 1.3246 - acc: 0.332 - ETA: 3s - loss: 1.3223 - acc: 0.336 - ETA: 3s - loss: 1.3197 - acc: 0.338 - ETA: 2s - loss: 1.3165 - acc: 0.341 - ETA: 2s - loss: 1.3151 - acc: 0.343 - ETA: 2s - loss: 1.3139 - acc: 0.344 - ETA: 1s - loss: 1.3126 - acc: 0.346 - ETA: 1s - loss: 1.3095 - acc: 0.348 - ETA: 0s - loss: 1.3081 - acc: 0.350 - ETA: 0s - loss: 1.3068 - acc: 0.351 - ETA: 0s - loss: 1.3048 - acc: 0.353 - 60s 23ms/step - loss: 1.3053 - acc: 0.3533 - val_loss: 1.2465 - val_acc: 0.4499\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24648, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/8\n",
      "2624/2624 [==============================] - ETA: 49s - loss: 1.2087 - acc: 0.45 - ETA: 50s - loss: 1.1569 - acc: 0.45 - ETA: 50s - loss: 1.2149 - acc: 0.41 - ETA: 50s - loss: 1.1797 - acc: 0.46 - ETA: 50s - loss: 1.1671 - acc: 0.48 - ETA: 49s - loss: 1.1498 - acc: 0.50 - ETA: 49s - loss: 1.0990 - acc: 0.54 - ETA: 49s - loss: 1.1396 - acc: 0.51 - ETA: 48s - loss: 1.1534 - acc: 0.49 - ETA: 48s - loss: 1.1413 - acc: 0.52 - ETA: 48s - loss: 1.1314 - acc: 0.52 - ETA: 47s - loss: 1.1305 - acc: 0.52 - ETA: 47s - loss: 1.1295 - acc: 0.53 - ETA: 46s - loss: 1.1120 - acc: 0.53 - ETA: 46s - loss: 1.1121 - acc: 0.53 - ETA: 46s - loss: 1.1119 - acc: 0.54 - ETA: 45s - loss: 1.1103 - acc: 0.53 - ETA: 45s - loss: 1.1072 - acc: 0.54 - ETA: 44s - loss: 1.1024 - acc: 0.53 - ETA: 44s - loss: 1.0985 - acc: 0.54 - ETA: 44s - loss: 1.1008 - acc: 0.53 - ETA: 43s - loss: 1.1306 - acc: 0.52 - ETA: 43s - loss: 1.1350 - acc: 0.52 - ETA: 42s - loss: 1.1269 - acc: 0.53 - ETA: 42s - loss: 1.1268 - acc: 0.53 - ETA: 41s - loss: 1.1223 - acc: 0.53 - ETA: 41s - loss: 1.1202 - acc: 0.53 - ETA: 41s - loss: 1.1107 - acc: 0.54 - ETA: 40s - loss: 1.1142 - acc: 0.54 - ETA: 40s - loss: 1.1167 - acc: 0.54 - ETA: 39s - loss: 1.1078 - acc: 0.55 - ETA: 39s - loss: 1.1011 - acc: 0.55 - ETA: 39s - loss: 1.0923 - acc: 0.56 - ETA: 38s - loss: 1.0968 - acc: 0.56 - ETA: 38s - loss: 1.0957 - acc: 0.56 - ETA: 37s - loss: 1.0963 - acc: 0.56 - ETA: 37s - loss: 1.0951 - acc: 0.56 - ETA: 37s - loss: 1.0964 - acc: 0.56 - ETA: 36s - loss: 1.0961 - acc: 0.56 - ETA: 36s - loss: 1.1010 - acc: 0.56 - ETA: 35s - loss: 1.0983 - acc: 0.56 - ETA: 35s - loss: 1.0919 - acc: 0.56 - ETA: 35s - loss: 1.0970 - acc: 0.56 - ETA: 34s - loss: 1.1003 - acc: 0.56 - ETA: 34s - loss: 1.1023 - acc: 0.55 - ETA: 34s - loss: 1.0995 - acc: 0.56 - ETA: 33s - loss: 1.0959 - acc: 0.56 - ETA: 33s - loss: 1.1002 - acc: 0.56 - ETA: 32s - loss: 1.1027 - acc: 0.55 - ETA: 32s - loss: 1.1003 - acc: 0.55 - ETA: 31s - loss: 1.0981 - acc: 0.55 - ETA: 31s - loss: 1.0964 - acc: 0.55 - ETA: 31s - loss: 1.0920 - acc: 0.55 - ETA: 30s - loss: 1.0952 - acc: 0.55 - ETA: 30s - loss: 1.0959 - acc: 0.55 - ETA: 29s - loss: 1.0901 - acc: 0.55 - ETA: 29s - loss: 1.0866 - acc: 0.55 - ETA: 29s - loss: 1.0892 - acc: 0.55 - ETA: 28s - loss: 1.0960 - acc: 0.55 - ETA: 28s - loss: 1.0952 - acc: 0.55 - ETA: 28s - loss: 1.0937 - acc: 0.54 - ETA: 27s - loss: 1.0916 - acc: 0.54 - ETA: 27s - loss: 1.0891 - acc: 0.54 - ETA: 26s - loss: 1.0868 - acc: 0.54 - ETA: 26s - loss: 1.0903 - acc: 0.54 - ETA: 26s - loss: 1.0948 - acc: 0.54 - ETA: 25s - loss: 1.0919 - acc: 0.54 - ETA: 25s - loss: 1.0933 - acc: 0.54 - ETA: 24s - loss: 1.0947 - acc: 0.53 - ETA: 24s - loss: 1.0923 - acc: 0.54 - ETA: 24s - loss: 1.0884 - acc: 0.54 - ETA: 23s - loss: 1.0861 - acc: 0.54 - ETA: 23s - loss: 1.0836 - acc: 0.54 - ETA: 22s - loss: 1.0811 - acc: 0.54 - ETA: 22s - loss: 1.0827 - acc: 0.54 - ETA: 22s - loss: 1.0864 - acc: 0.53 - ETA: 21s - loss: 1.0843 - acc: 0.54 - ETA: 21s - loss: 1.0833 - acc: 0.54 - ETA: 20s - loss: 1.0834 - acc: 0.54 - ETA: 20s - loss: 1.0793 - acc: 0.54 - ETA: 20s - loss: 1.0767 - acc: 0.54 - ETA: 19s - loss: 1.0792 - acc: 0.54 - ETA: 19s - loss: 1.0780 - acc: 0.54 - ETA: 18s - loss: 1.0772 - acc: 0.54 - ETA: 18s - loss: 1.0757 - acc: 0.54 - ETA: 18s - loss: 1.0745 - acc: 0.54 - ETA: 17s - loss: 1.0751 - acc: 0.54 - ETA: 17s - loss: 1.0760 - acc: 0.54 - ETA: 16s - loss: 1.0754 - acc: 0.54 - ETA: 16s - loss: 1.0798 - acc: 0.54 - ETA: 16s - loss: 1.0782 - acc: 0.54 - ETA: 15s - loss: 1.0784 - acc: 0.54 - ETA: 15s - loss: 1.0760 - acc: 0.54 - ETA: 14s - loss: 1.0734 - acc: 0.55 - ETA: 14s - loss: 1.0740 - acc: 0.55 - ETA: 14s - loss: 1.0731 - acc: 0.55 - ETA: 13s - loss: 1.0719 - acc: 0.55 - ETA: 13s - loss: 1.0735 - acc: 0.55 - ETA: 12s - loss: 1.0745 - acc: 0.54 - ETA: 12s - loss: 1.0728 - acc: 0.55 - ETA: 12s - loss: 1.0721 - acc: 0.55 - ETA: 11s - loss: 1.0696 - acc: 0.55 - ETA: 11s - loss: 1.0687 - acc: 0.55 - ETA: 10s - loss: 1.0668 - acc: 0.55 - ETA: 10s - loss: 1.0648 - acc: 0.55 - ETA: 10s - loss: 1.0619 - acc: 0.55 - ETA: 9s - loss: 1.0628 - acc: 0.5542 - ETA: 9s - loss: 1.0631 - acc: 0.553 - ETA: 8s - loss: 1.0617 - acc: 0.552 - ETA: 8s - loss: 1.0610 - acc: 0.553 - ETA: 8s - loss: 1.0642 - acc: 0.552 - ETA: 7s - loss: 1.0655 - acc: 0.550 - ETA: 7s - loss: 1.0656 - acc: 0.550 - ETA: 6s - loss: 1.0649 - acc: 0.549 - ETA: 6s - loss: 1.0663 - acc: 0.549 - ETA: 6s - loss: 1.0661 - acc: 0.550 - ETA: 5s - loss: 1.0631 - acc: 0.552 - ETA: 5s - loss: 1.0611 - acc: 0.553 - ETA: 4s - loss: 1.0593 - acc: 0.553 - ETA: 4s - loss: 1.0606 - acc: 0.552 - ETA: 4s - loss: 1.0601 - acc: 0.553 - ETA: 3s - loss: 1.0587 - acc: 0.554 - ETA: 3s - loss: 1.0590 - acc: 0.553 - ETA: 2s - loss: 1.0589 - acc: 0.553 - ETA: 2s - loss: 1.0576 - acc: 0.553 - ETA: 2s - loss: 1.0552 - acc: 0.555 - ETA: 1s - loss: 1.0567 - acc: 0.555 - ETA: 1s - loss: 1.0564 - acc: 0.555 - ETA: 0s - loss: 1.0547 - acc: 0.555 - ETA: 0s - loss: 1.0562 - acc: 0.554 - ETA: 0s - loss: 1.0573 - acc: 0.553 - 59s 23ms/step - loss: 1.0568 - acc: 0.5541 - val_loss: 1.0107 - val_acc: 0.5320\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.24648 to 1.01071, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624/2624 [==============================] - ETA: 52s - loss: 0.7006 - acc: 0.80 - ETA: 52s - loss: 0.7447 - acc: 0.77 - ETA: 52s - loss: 0.8250 - acc: 0.68 - ETA: 51s - loss: 0.8985 - acc: 0.63 - ETA: 51s - loss: 0.8990 - acc: 0.64 - ETA: 50s - loss: 0.8459 - acc: 0.65 - ETA: 49s - loss: 0.8542 - acc: 0.65 - ETA: 49s - loss: 0.8239 - acc: 0.67 - ETA: 49s - loss: 0.8151 - acc: 0.68 - ETA: 48s - loss: 0.8571 - acc: 0.67 - ETA: 48s - loss: 0.8648 - acc: 0.66 - ETA: 47s - loss: 0.8760 - acc: 0.66 - ETA: 47s - loss: 0.8660 - acc: 0.67 - ETA: 46s - loss: 0.8922 - acc: 0.66 - ETA: 46s - loss: 0.9201 - acc: 0.64 - ETA: 46s - loss: 0.9198 - acc: 0.65 - ETA: 45s - loss: 0.9189 - acc: 0.64 - ETA: 45s - loss: 0.9243 - acc: 0.63 - ETA: 44s - loss: 0.9236 - acc: 0.64 - ETA: 44s - loss: 0.9446 - acc: 0.63 - ETA: 44s - loss: 0.9546 - acc: 0.62 - ETA: 43s - loss: 0.9555 - acc: 0.61 - ETA: 43s - loss: 0.9505 - acc: 0.61 - ETA: 42s - loss: 0.9430 - acc: 0.62 - ETA: 42s - loss: 0.9347 - acc: 0.62 - ETA: 42s - loss: 0.9276 - acc: 0.62 - ETA: 41s - loss: 0.9224 - acc: 0.62 - ETA: 41s - loss: 0.9223 - acc: 0.63 - ETA: 41s - loss: 0.9301 - acc: 0.62 - ETA: 40s - loss: 0.9272 - acc: 0.62 - ETA: 40s - loss: 0.9312 - acc: 0.62 - ETA: 39s - loss: 0.9319 - acc: 0.62 - ETA: 39s - loss: 0.9261 - acc: 0.63 - ETA: 38s - loss: 0.9180 - acc: 0.63 - ETA: 38s - loss: 0.9231 - acc: 0.63 - ETA: 38s - loss: 0.9247 - acc: 0.63 - ETA: 37s - loss: 0.9346 - acc: 0.62 - ETA: 37s - loss: 0.9328 - acc: 0.62 - ETA: 36s - loss: 0.9350 - acc: 0.62 - ETA: 36s - loss: 0.9338 - acc: 0.62 - ETA: 36s - loss: 0.9358 - acc: 0.62 - ETA: 35s - loss: 0.9388 - acc: 0.62 - ETA: 35s - loss: 0.9348 - acc: 0.61 - ETA: 34s - loss: 0.9345 - acc: 0.61 - ETA: 34s - loss: 0.9337 - acc: 0.61 - ETA: 34s - loss: 0.9302 - acc: 0.61 - ETA: 33s - loss: 0.9384 - acc: 0.61 - ETA: 33s - loss: 0.9411 - acc: 0.61 - ETA: 32s - loss: 0.9412 - acc: 0.61 - ETA: 32s - loss: 0.9391 - acc: 0.61 - ETA: 32s - loss: 0.9383 - acc: 0.61 - ETA: 31s - loss: 0.9391 - acc: 0.60 - ETA: 31s - loss: 0.9373 - acc: 0.61 - ETA: 30s - loss: 0.9356 - acc: 0.61 - ETA: 30s - loss: 0.9361 - acc: 0.61 - ETA: 30s - loss: 0.9328 - acc: 0.61 - ETA: 29s - loss: 0.9308 - acc: 0.61 - ETA: 29s - loss: 0.9279 - acc: 0.61 - ETA: 28s - loss: 0.9266 - acc: 0.61 - ETA: 28s - loss: 0.9210 - acc: 0.62 - ETA: 28s - loss: 0.9231 - acc: 0.62 - ETA: 27s - loss: 0.9195 - acc: 0.62 - ETA: 27s - loss: 0.9350 - acc: 0.61 - ETA: 26s - loss: 0.9335 - acc: 0.61 - ETA: 26s - loss: 0.9309 - acc: 0.61 - ETA: 26s - loss: 0.9318 - acc: 0.61 - ETA: 25s - loss: 0.9306 - acc: 0.61 - ETA: 25s - loss: 0.9291 - acc: 0.61 - ETA: 24s - loss: 0.9292 - acc: 0.61 - ETA: 24s - loss: 0.9288 - acc: 0.61 - ETA: 24s - loss: 0.9265 - acc: 0.61 - ETA: 23s - loss: 0.9212 - acc: 0.62 - ETA: 23s - loss: 0.9181 - acc: 0.62 - ETA: 22s - loss: 0.9188 - acc: 0.62 - ETA: 22s - loss: 0.9222 - acc: 0.62 - ETA: 22s - loss: 0.9204 - acc: 0.62 - ETA: 21s - loss: 0.9215 - acc: 0.62 - ETA: 21s - loss: 0.9247 - acc: 0.62 - ETA: 20s - loss: 0.9214 - acc: 0.62 - ETA: 20s - loss: 0.9199 - acc: 0.62 - ETA: 20s - loss: 0.9169 - acc: 0.62 - ETA: 19s - loss: 0.9157 - acc: 0.62 - ETA: 19s - loss: 0.9153 - acc: 0.62 - ETA: 18s - loss: 0.9172 - acc: 0.62 - ETA: 18s - loss: 0.9187 - acc: 0.62 - ETA: 18s - loss: 0.9182 - acc: 0.62 - ETA: 17s - loss: 0.9165 - acc: 0.62 - ETA: 17s - loss: 0.9176 - acc: 0.62 - ETA: 16s - loss: 0.9162 - acc: 0.62 - ETA: 16s - loss: 0.9143 - acc: 0.63 - ETA: 16s - loss: 0.9118 - acc: 0.63 - ETA: 15s - loss: 0.9094 - acc: 0.63 - ETA: 15s - loss: 0.9130 - acc: 0.62 - ETA: 14s - loss: 0.9137 - acc: 0.62 - ETA: 14s - loss: 0.9136 - acc: 0.62 - ETA: 14s - loss: 0.9128 - acc: 0.62 - ETA: 13s - loss: 0.9131 - acc: 0.62 - ETA: 13s - loss: 0.9103 - acc: 0.62 - ETA: 12s - loss: 0.9095 - acc: 0.62 - ETA: 12s - loss: 0.9097 - acc: 0.62 - ETA: 12s - loss: 0.9095 - acc: 0.62 - ETA: 11s - loss: 0.9079 - acc: 0.63 - ETA: 11s - loss: 0.9072 - acc: 0.63 - ETA: 10s - loss: 0.9067 - acc: 0.63 - ETA: 10s - loss: 0.9105 - acc: 0.63 - ETA: 10s - loss: 0.9114 - acc: 0.62 - ETA: 9s - loss: 0.9113 - acc: 0.6280 - ETA: 9s - loss: 0.9083 - acc: 0.629 - ETA: 8s - loss: 0.9071 - acc: 0.630 - ETA: 8s - loss: 0.9062 - acc: 0.630 - ETA: 8s - loss: 0.9060 - acc: 0.630 - ETA: 7s - loss: 0.9041 - acc: 0.631 - ETA: 7s - loss: 0.9023 - acc: 0.632 - ETA: 6s - loss: 0.9062 - acc: 0.632 - ETA: 6s - loss: 0.9046 - acc: 0.633 - ETA: 6s - loss: 0.9066 - acc: 0.631 - ETA: 5s - loss: 0.9055 - acc: 0.631 - ETA: 5s - loss: 0.9102 - acc: 0.631 - ETA: 4s - loss: 0.9138 - acc: 0.629 - ETA: 4s - loss: 0.9132 - acc: 0.629 - ETA: 4s - loss: 0.9140 - acc: 0.628 - ETA: 3s - loss: 0.9149 - acc: 0.628 - ETA: 3s - loss: 0.9147 - acc: 0.628 - ETA: 2s - loss: 0.9130 - acc: 0.629 - ETA: 2s - loss: 0.9111 - acc: 0.630 - ETA: 2s - loss: 0.9116 - acc: 0.629 - ETA: 1s - loss: 0.9127 - acc: 0.629 - ETA: 1s - loss: 0.9123 - acc: 0.628 - ETA: 0s - loss: 0.9122 - acc: 0.628 - ETA: 0s - loss: 0.9116 - acc: 0.627 - ETA: 0s - loss: 0.9111 - acc: 0.628 - 59s 22ms/step - loss: 0.9105 - acc: 0.6288 - val_loss: 1.0124 - val_acc: 0.5267\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.01071\n",
      "Epoch 4/8\n",
      "2624/2624 [==============================] - ETA: 49s - loss: 1.2470 - acc: 0.45 - ETA: 50s - loss: 1.0800 - acc: 0.52 - ETA: 50s - loss: 0.9767 - acc: 0.60 - ETA: 50s - loss: 0.8982 - acc: 0.65 - ETA: 49s - loss: 0.8632 - acc: 0.66 - ETA: 49s - loss: 0.9406 - acc: 0.61 - ETA: 49s - loss: 0.9298 - acc: 0.62 - ETA: 48s - loss: 0.9516 - acc: 0.61 - ETA: 48s - loss: 0.9490 - acc: 0.60 - ETA: 48s - loss: 0.9069 - acc: 0.63 - ETA: 48s - loss: 0.9075 - acc: 0.63 - ETA: 47s - loss: 0.9079 - acc: 0.63 - ETA: 47s - loss: 0.9052 - acc: 0.63 - ETA: 46s - loss: 0.8897 - acc: 0.63 - ETA: 46s - loss: 0.8878 - acc: 0.64 - ETA: 45s - loss: 0.8784 - acc: 0.64 - ETA: 45s - loss: 0.8545 - acc: 0.65 - ETA: 45s - loss: 0.8357 - acc: 0.67 - ETA: 44s - loss: 0.8203 - acc: 0.67 - ETA: 44s - loss: 0.8143 - acc: 0.68 - ETA: 43s - loss: 0.8159 - acc: 0.67 - ETA: 43s - loss: 0.8149 - acc: 0.66 - ETA: 43s - loss: 0.8212 - acc: 0.66 - ETA: 42s - loss: 0.8295 - acc: 0.65 - ETA: 42s - loss: 0.8282 - acc: 0.65 - ETA: 41s - loss: 0.8344 - acc: 0.65 - ETA: 41s - loss: 0.8383 - acc: 0.65 - ETA: 41s - loss: 0.8416 - acc: 0.64 - ETA: 40s - loss: 0.8332 - acc: 0.65 - ETA: 40s - loss: 0.8268 - acc: 0.65 - ETA: 40s - loss: 0.8134 - acc: 0.66 - ETA: 39s - loss: 0.8146 - acc: 0.67 - ETA: 39s - loss: 0.8116 - acc: 0.66 - ETA: 38s - loss: 0.8023 - acc: 0.67 - ETA: 38s - loss: 0.7992 - acc: 0.67 - ETA: 38s - loss: 0.8122 - acc: 0.67 - ETA: 37s - loss: 0.8158 - acc: 0.67 - ETA: 37s - loss: 0.8116 - acc: 0.67 - ETA: 36s - loss: 0.8062 - acc: 0.67 - ETA: 36s - loss: 0.8005 - acc: 0.67 - ETA: 36s - loss: 0.8109 - acc: 0.67 - ETA: 35s - loss: 0.8135 - acc: 0.67 - ETA: 35s - loss: 0.8117 - acc: 0.67 - ETA: 34s - loss: 0.8050 - acc: 0.67 - ETA: 34s - loss: 0.8020 - acc: 0.67 - ETA: 33s - loss: 0.7989 - acc: 0.67 - ETA: 33s - loss: 0.7957 - acc: 0.68 - ETA: 33s - loss: 0.7925 - acc: 0.67 - ETA: 32s - loss: 0.7928 - acc: 0.67 - ETA: 32s - loss: 0.7904 - acc: 0.67 - ETA: 31s - loss: 0.7858 - acc: 0.68 - ETA: 31s - loss: 0.7851 - acc: 0.67 - ETA: 31s - loss: 0.7847 - acc: 0.67 - ETA: 30s - loss: 0.7877 - acc: 0.67 - ETA: 30s - loss: 0.7912 - acc: 0.67 - ETA: 29s - loss: 0.7877 - acc: 0.67 - ETA: 29s - loss: 0.7863 - acc: 0.68 - ETA: 29s - loss: 0.7810 - acc: 0.68 - ETA: 28s - loss: 0.7820 - acc: 0.68 - ETA: 28s - loss: 0.7797 - acc: 0.68 - ETA: 27s - loss: 0.7769 - acc: 0.68 - ETA: 27s - loss: 0.7742 - acc: 0.68 - ETA: 27s - loss: 0.7794 - acc: 0.67 - ETA: 26s - loss: 0.7823 - acc: 0.67 - ETA: 26s - loss: 0.7833 - acc: 0.67 - ETA: 25s - loss: 0.7808 - acc: 0.67 - ETA: 25s - loss: 0.7806 - acc: 0.67 - ETA: 25s - loss: 0.7880 - acc: 0.67 - ETA: 24s - loss: 0.7903 - acc: 0.67 - ETA: 24s - loss: 0.7911 - acc: 0.67 - ETA: 23s - loss: 0.7889 - acc: 0.67 - ETA: 23s - loss: 0.7860 - acc: 0.67 - ETA: 23s - loss: 0.7929 - acc: 0.67 - ETA: 22s - loss: 0.7923 - acc: 0.67 - ETA: 22s - loss: 0.7892 - acc: 0.67 - ETA: 21s - loss: 0.7920 - acc: 0.67 - ETA: 21s - loss: 0.7990 - acc: 0.67 - ETA: 21s - loss: 0.8040 - acc: 0.67 - ETA: 20s - loss: 0.8058 - acc: 0.67 - ETA: 20s - loss: 0.8036 - acc: 0.67 - ETA: 19s - loss: 0.8017 - acc: 0.67 - ETA: 19s - loss: 0.7978 - acc: 0.67 - ETA: 19s - loss: 0.7963 - acc: 0.67 - ETA: 18s - loss: 0.7983 - acc: 0.67 - ETA: 18s - loss: 0.7955 - acc: 0.67 - ETA: 18s - loss: 0.7999 - acc: 0.67 - ETA: 17s - loss: 0.8095 - acc: 0.67 - ETA: 17s - loss: 0.8099 - acc: 0.67 - ETA: 16s - loss: 0.8106 - acc: 0.67 - ETA: 16s - loss: 0.8104 - acc: 0.67 - ETA: 16s - loss: 0.8083 - acc: 0.67 - ETA: 15s - loss: 0.8050 - acc: 0.67 - ETA: 15s - loss: 0.8035 - acc: 0.67 - ETA: 14s - loss: 0.8002 - acc: 0.67 - ETA: 14s - loss: 0.8014 - acc: 0.67 - ETA: 14s - loss: 0.8007 - acc: 0.67 - ETA: 13s - loss: 0.7982 - acc: 0.68 - ETA: 13s - loss: 0.7961 - acc: 0.68 - ETA: 12s - loss: 0.7936 - acc: 0.68 - ETA: 12s - loss: 0.7960 - acc: 0.68 - ETA: 12s - loss: 0.7955 - acc: 0.68 - ETA: 11s - loss: 0.7968 - acc: 0.68 - ETA: 11s - loss: 0.7944 - acc: 0.68 - ETA: 10s - loss: 0.7947 - acc: 0.68 - ETA: 10s - loss: 0.7953 - acc: 0.68 - ETA: 10s - loss: 0.7927 - acc: 0.68 - ETA: 9s - loss: 0.7923 - acc: 0.6836 - ETA: 9s - loss: 0.7919 - acc: 0.683 - ETA: 8s - loss: 0.7922 - acc: 0.683 - ETA: 8s - loss: 0.7918 - acc: 0.683 - ETA: 8s - loss: 0.7906 - acc: 0.684 - ETA: 7s - loss: 0.7890 - acc: 0.684 - ETA: 7s - loss: 0.7896 - acc: 0.684 - ETA: 6s - loss: 0.7904 - acc: 0.685 - ETA: 6s - loss: 0.7872 - acc: 0.687 - ETA: 6s - loss: 0.7865 - acc: 0.687 - ETA: 5s - loss: 0.7844 - acc: 0.688 - ETA: 5s - loss: 0.7822 - acc: 0.689 - ETA: 4s - loss: 0.7799 - acc: 0.690 - ETA: 4s - loss: 0.7800 - acc: 0.690 - ETA: 4s - loss: 0.7772 - acc: 0.691 - ETA: 3s - loss: 0.7778 - acc: 0.691 - ETA: 3s - loss: 0.7793 - acc: 0.691 - ETA: 2s - loss: 0.7816 - acc: 0.689 - ETA: 2s - loss: 0.7859 - acc: 0.688 - ETA: 2s - loss: 0.7862 - acc: 0.687 - ETA: 1s - loss: 0.7851 - acc: 0.688 - ETA: 1s - loss: 0.7827 - acc: 0.689 - ETA: 0s - loss: 0.7822 - acc: 0.689 - ETA: 0s - loss: 0.7838 - acc: 0.688 - ETA: 0s - loss: 0.7822 - acc: 0.688 - 59s 22ms/step - loss: 0.7813 - acc: 0.6886 - val_loss: 1.2494 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.01071\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624/2624 [==============================] - ETA: 50s - loss: 0.7521 - acc: 0.80 - ETA: 50s - loss: 0.7646 - acc: 0.70 - ETA: 50s - loss: 0.7052 - acc: 0.75 - ETA: 50s - loss: 0.6837 - acc: 0.73 - ETA: 50s - loss: 0.7045 - acc: 0.72 - ETA: 49s - loss: 0.7108 - acc: 0.72 - ETA: 49s - loss: 0.6760 - acc: 0.75 - ETA: 48s - loss: 0.6635 - acc: 0.74 - ETA: 48s - loss: 0.6473 - acc: 0.75 - ETA: 48s - loss: 0.6587 - acc: 0.74 - ETA: 47s - loss: 0.6598 - acc: 0.75 - ETA: 47s - loss: 0.6618 - acc: 0.75 - ETA: 47s - loss: 0.6558 - acc: 0.75 - ETA: 46s - loss: 0.6339 - acc: 0.76 - ETA: 46s - loss: 0.6202 - acc: 0.76 - ETA: 46s - loss: 0.6139 - acc: 0.76 - ETA: 45s - loss: 0.6211 - acc: 0.75 - ETA: 45s - loss: 0.6209 - acc: 0.75 - ETA: 44s - loss: 0.6253 - acc: 0.75 - ETA: 44s - loss: 0.6196 - acc: 0.75 - ETA: 43s - loss: 0.6076 - acc: 0.75 - ETA: 43s - loss: 0.6346 - acc: 0.74 - ETA: 43s - loss: 0.6419 - acc: 0.73 - ETA: 42s - loss: 0.6421 - acc: 0.73 - ETA: 42s - loss: 0.6340 - acc: 0.74 - ETA: 41s - loss: 0.6453 - acc: 0.74 - ETA: 41s - loss: 0.6460 - acc: 0.74 - ETA: 41s - loss: 0.6473 - acc: 0.74 - ETA: 40s - loss: 0.6552 - acc: 0.74 - ETA: 40s - loss: 0.6545 - acc: 0.74 - ETA: 39s - loss: 0.6454 - acc: 0.74 - ETA: 39s - loss: 0.6422 - acc: 0.74 - ETA: 39s - loss: 0.6419 - acc: 0.75 - ETA: 38s - loss: 0.6368 - acc: 0.75 - ETA: 38s - loss: 0.6461 - acc: 0.75 - ETA: 37s - loss: 0.6531 - acc: 0.75 - ETA: 37s - loss: 0.6615 - acc: 0.74 - ETA: 37s - loss: 0.6639 - acc: 0.74 - ETA: 36s - loss: 0.6625 - acc: 0.73 - ETA: 36s - loss: 0.6557 - acc: 0.74 - ETA: 35s - loss: 0.6528 - acc: 0.74 - ETA: 35s - loss: 0.6463 - acc: 0.74 - ETA: 35s - loss: 0.6512 - acc: 0.74 - ETA: 34s - loss: 0.6469 - acc: 0.74 - ETA: 34s - loss: 0.6420 - acc: 0.74 - ETA: 33s - loss: 0.6451 - acc: 0.74 - ETA: 33s - loss: 0.6464 - acc: 0.74 - ETA: 33s - loss: 0.6520 - acc: 0.74 - ETA: 32s - loss: 0.6509 - acc: 0.74 - ETA: 32s - loss: 0.6526 - acc: 0.74 - ETA: 31s - loss: 0.6494 - acc: 0.74 - ETA: 31s - loss: 0.6559 - acc: 0.73 - ETA: 31s - loss: 0.6610 - acc: 0.73 - ETA: 30s - loss: 0.6604 - acc: 0.73 - ETA: 30s - loss: 0.6618 - acc: 0.73 - ETA: 29s - loss: 0.6622 - acc: 0.73 - ETA: 29s - loss: 0.6596 - acc: 0.73 - ETA: 29s - loss: 0.6617 - acc: 0.73 - ETA: 28s - loss: 0.6647 - acc: 0.73 - ETA: 28s - loss: 0.6642 - acc: 0.73 - ETA: 27s - loss: 0.6684 - acc: 0.73 - ETA: 27s - loss: 0.6703 - acc: 0.72 - ETA: 27s - loss: 0.6682 - acc: 0.73 - ETA: 26s - loss: 0.6642 - acc: 0.73 - ETA: 26s - loss: 0.6575 - acc: 0.73 - ETA: 25s - loss: 0.6570 - acc: 0.73 - ETA: 25s - loss: 0.6585 - acc: 0.73 - ETA: 25s - loss: 0.6566 - acc: 0.73 - ETA: 24s - loss: 0.6537 - acc: 0.74 - ETA: 24s - loss: 0.6584 - acc: 0.74 - ETA: 23s - loss: 0.6655 - acc: 0.73 - ETA: 23s - loss: 0.6683 - acc: 0.73 - ETA: 23s - loss: 0.6675 - acc: 0.74 - ETA: 22s - loss: 0.6646 - acc: 0.74 - ETA: 22s - loss: 0.6646 - acc: 0.74 - ETA: 21s - loss: 0.6644 - acc: 0.74 - ETA: 21s - loss: 0.6624 - acc: 0.74 - ETA: 21s - loss: 0.6623 - acc: 0.74 - ETA: 20s - loss: 0.6645 - acc: 0.74 - ETA: 20s - loss: 0.6716 - acc: 0.74 - ETA: 19s - loss: 0.6737 - acc: 0.74 - ETA: 19s - loss: 0.6762 - acc: 0.74 - ETA: 19s - loss: 0.6746 - acc: 0.74 - ETA: 18s - loss: 0.6748 - acc: 0.74 - ETA: 18s - loss: 0.6735 - acc: 0.74 - ETA: 17s - loss: 0.6717 - acc: 0.74 - ETA: 17s - loss: 0.6701 - acc: 0.74 - ETA: 17s - loss: 0.6736 - acc: 0.74 - ETA: 16s - loss: 0.6722 - acc: 0.74 - ETA: 16s - loss: 0.6686 - acc: 0.74 - ETA: 16s - loss: 0.6685 - acc: 0.74 - ETA: 15s - loss: 0.6675 - acc: 0.74 - ETA: 15s - loss: 0.6683 - acc: 0.74 - ETA: 14s - loss: 0.6662 - acc: 0.74 - ETA: 14s - loss: 0.6656 - acc: 0.74 - ETA: 14s - loss: 0.6682 - acc: 0.74 - ETA: 13s - loss: 0.6756 - acc: 0.73 - ETA: 13s - loss: 0.6761 - acc: 0.73 - ETA: 12s - loss: 0.6769 - acc: 0.74 - ETA: 12s - loss: 0.6788 - acc: 0.73 - ETA: 12s - loss: 0.6781 - acc: 0.73 - ETA: 11s - loss: 0.6760 - acc: 0.73 - ETA: 11s - loss: 0.6734 - acc: 0.73 - ETA: 10s - loss: 0.6713 - acc: 0.74 - ETA: 10s - loss: 0.6703 - acc: 0.74 - ETA: 10s - loss: 0.6688 - acc: 0.74 - ETA: 9s - loss: 0.6675 - acc: 0.7407 - ETA: 9s - loss: 0.6663 - acc: 0.742 - ETA: 8s - loss: 0.6631 - acc: 0.743 - ETA: 8s - loss: 0.6653 - acc: 0.742 - ETA: 8s - loss: 0.6676 - acc: 0.741 - ETA: 7s - loss: 0.6676 - acc: 0.740 - ETA: 7s - loss: 0.6642 - acc: 0.742 - ETA: 6s - loss: 0.6615 - acc: 0.744 - ETA: 6s - loss: 0.6652 - acc: 0.742 - ETA: 6s - loss: 0.6674 - acc: 0.741 - ETA: 5s - loss: 0.6683 - acc: 0.741 - ETA: 5s - loss: 0.6689 - acc: 0.741 - ETA: 4s - loss: 0.6672 - acc: 0.742 - ETA: 4s - loss: 0.6666 - acc: 0.742 - ETA: 4s - loss: 0.6631 - acc: 0.743 - ETA: 3s - loss: 0.6604 - acc: 0.744 - ETA: 3s - loss: 0.6590 - acc: 0.744 - ETA: 2s - loss: 0.6586 - acc: 0.745 - ETA: 2s - loss: 0.6580 - acc: 0.746 - ETA: 2s - loss: 0.6574 - acc: 0.746 - ETA: 1s - loss: 0.6545 - acc: 0.747 - ETA: 1s - loss: 0.6550 - acc: 0.747 - ETA: 0s - loss: 0.6534 - acc: 0.748 - ETA: 0s - loss: 0.6544 - acc: 0.748 - ETA: 0s - loss: 0.6570 - acc: 0.746 - 59s 22ms/step - loss: 0.6566 - acc: 0.7466 - val_loss: 2.2952 - val_acc: 0.4072\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.01071\n",
      "Epoch 6/8\n",
      "2624/2624 [==============================] - ETA: 50s - loss: 0.9791 - acc: 0.75 - ETA: 50s - loss: 0.7742 - acc: 0.80 - ETA: 50s - loss: 0.6151 - acc: 0.85 - ETA: 50s - loss: 0.6026 - acc: 0.85 - ETA: 49s - loss: 0.5603 - acc: 0.86 - ETA: 49s - loss: 0.5655 - acc: 0.85 - ETA: 49s - loss: 0.5337 - acc: 0.87 - ETA: 48s - loss: 0.5372 - acc: 0.84 - ETA: 48s - loss: 0.5232 - acc: 0.84 - ETA: 47s - loss: 0.5678 - acc: 0.83 - ETA: 47s - loss: 0.5773 - acc: 0.81 - ETA: 47s - loss: 0.5679 - acc: 0.82 - ETA: 46s - loss: 0.5622 - acc: 0.82 - ETA: 46s - loss: 0.5661 - acc: 0.82 - ETA: 45s - loss: 0.5868 - acc: 0.81 - ETA: 45s - loss: 0.5908 - acc: 0.80 - ETA: 45s - loss: 0.5782 - acc: 0.80 - ETA: 44s - loss: 0.5648 - acc: 0.81 - ETA: 44s - loss: 0.5613 - acc: 0.81 - ETA: 44s - loss: 0.5797 - acc: 0.80 - ETA: 43s - loss: 0.5990 - acc: 0.79 - ETA: 43s - loss: 0.6142 - acc: 0.78 - ETA: 42s - loss: 0.6129 - acc: 0.78 - ETA: 42s - loss: 0.6091 - acc: 0.78 - ETA: 42s - loss: 0.6012 - acc: 0.79 - ETA: 41s - loss: 0.5904 - acc: 0.79 - ETA: 41s - loss: 0.5838 - acc: 0.79 - ETA: 40s - loss: 0.5740 - acc: 0.79 - ETA: 40s - loss: 0.5688 - acc: 0.79 - ETA: 40s - loss: 0.5700 - acc: 0.79 - ETA: 39s - loss: 0.5762 - acc: 0.79 - ETA: 39s - loss: 0.5746 - acc: 0.79 - ETA: 38s - loss: 0.5729 - acc: 0.79 - ETA: 38s - loss: 0.5718 - acc: 0.79 - ETA: 38s - loss: 0.5645 - acc: 0.79 - ETA: 37s - loss: 0.5610 - acc: 0.79 - ETA: 37s - loss: 0.5579 - acc: 0.79 - ETA: 36s - loss: 0.5566 - acc: 0.79 - ETA: 36s - loss: 0.5575 - acc: 0.79 - ETA: 36s - loss: 0.5623 - acc: 0.79 - ETA: 35s - loss: 0.5594 - acc: 0.79 - ETA: 35s - loss: 0.5537 - acc: 0.79 - ETA: 34s - loss: 0.5486 - acc: 0.79 - ETA: 34s - loss: 0.5446 - acc: 0.79 - ETA: 34s - loss: 0.5484 - acc: 0.79 - ETA: 33s - loss: 0.5486 - acc: 0.79 - ETA: 33s - loss: 0.5450 - acc: 0.79 - ETA: 32s - loss: 0.5558 - acc: 0.79 - ETA: 32s - loss: 0.5547 - acc: 0.79 - ETA: 32s - loss: 0.5556 - acc: 0.79 - ETA: 31s - loss: 0.5640 - acc: 0.78 - ETA: 31s - loss: 0.5612 - acc: 0.78 - ETA: 30s - loss: 0.5574 - acc: 0.79 - ETA: 30s - loss: 0.5590 - acc: 0.79 - ETA: 30s - loss: 0.5560 - acc: 0.79 - ETA: 29s - loss: 0.5527 - acc: 0.79 - ETA: 29s - loss: 0.5531 - acc: 0.79 - ETA: 28s - loss: 0.5586 - acc: 0.79 - ETA: 28s - loss: 0.5683 - acc: 0.78 - ETA: 28s - loss: 0.5671 - acc: 0.79 - ETA: 27s - loss: 0.5636 - acc: 0.79 - ETA: 27s - loss: 0.5627 - acc: 0.79 - ETA: 26s - loss: 0.5613 - acc: 0.79 - ETA: 26s - loss: 0.5548 - acc: 0.79 - ETA: 26s - loss: 0.5551 - acc: 0.79 - ETA: 25s - loss: 0.5565 - acc: 0.79 - ETA: 25s - loss: 0.5594 - acc: 0.79 - ETA: 25s - loss: 0.5621 - acc: 0.79 - ETA: 24s - loss: 0.5635 - acc: 0.79 - ETA: 24s - loss: 0.5652 - acc: 0.79 - ETA: 23s - loss: 0.5644 - acc: 0.79 - ETA: 23s - loss: 0.5637 - acc: 0.79 - ETA: 23s - loss: 0.5611 - acc: 0.79 - ETA: 22s - loss: 0.5603 - acc: 0.79 - ETA: 22s - loss: 0.5567 - acc: 0.79 - ETA: 21s - loss: 0.5589 - acc: 0.79 - ETA: 21s - loss: 0.5614 - acc: 0.79 - ETA: 21s - loss: 0.5611 - acc: 0.79 - ETA: 20s - loss: 0.5574 - acc: 0.79 - ETA: 20s - loss: 0.5551 - acc: 0.79 - ETA: 19s - loss: 0.5546 - acc: 0.79 - ETA: 19s - loss: 0.5528 - acc: 0.79 - ETA: 19s - loss: 0.5551 - acc: 0.79 - ETA: 18s - loss: 0.5546 - acc: 0.79 - ETA: 18s - loss: 0.5536 - acc: 0.79 - ETA: 17s - loss: 0.5525 - acc: 0.79 - ETA: 17s - loss: 0.5511 - acc: 0.79 - ETA: 17s - loss: 0.5477 - acc: 0.79 - ETA: 16s - loss: 0.5440 - acc: 0.79 - ETA: 16s - loss: 0.5439 - acc: 0.80 - ETA: 15s - loss: 0.5462 - acc: 0.79 - ETA: 15s - loss: 0.5540 - acc: 0.79 - ETA: 15s - loss: 0.5565 - acc: 0.79 - ETA: 14s - loss: 0.5556 - acc: 0.79 - ETA: 14s - loss: 0.5535 - acc: 0.79 - ETA: 13s - loss: 0.5508 - acc: 0.79 - ETA: 13s - loss: 0.5490 - acc: 0.79 - ETA: 13s - loss: 0.5489 - acc: 0.79 - ETA: 12s - loss: 0.5491 - acc: 0.79 - ETA: 12s - loss: 0.5475 - acc: 0.79 - ETA: 11s - loss: 0.5469 - acc: 0.79 - ETA: 11s - loss: 0.5432 - acc: 0.80 - ETA: 11s - loss: 0.5391 - acc: 0.80 - ETA: 10s - loss: 0.5426 - acc: 0.80 - ETA: 10s - loss: 0.5422 - acc: 0.80 - ETA: 9s - loss: 0.5447 - acc: 0.8024 - ETA: 9s - loss: 0.5444 - acc: 0.802 - ETA: 9s - loss: 0.5450 - acc: 0.802 - ETA: 8s - loss: 0.5479 - acc: 0.801 - ETA: 8s - loss: 0.5497 - acc: 0.799 - ETA: 7s - loss: 0.5518 - acc: 0.797 - ETA: 7s - loss: 0.5504 - acc: 0.797 - ETA: 7s - loss: 0.5495 - acc: 0.797 - ETA: 6s - loss: 0.5494 - acc: 0.796 - ETA: 6s - loss: 0.5499 - acc: 0.796 - ETA: 6s - loss: 0.5469 - acc: 0.797 - ETA: 5s - loss: 0.5464 - acc: 0.797 - ETA: 5s - loss: 0.5470 - acc: 0.796 - ETA: 4s - loss: 0.5477 - acc: 0.796 - ETA: 4s - loss: 0.5481 - acc: 0.796 - ETA: 4s - loss: 0.5473 - acc: 0.797 - ETA: 3s - loss: 0.5462 - acc: 0.797 - ETA: 3s - loss: 0.5455 - acc: 0.797 - ETA: 2s - loss: 0.5481 - acc: 0.797 - ETA: 2s - loss: 0.5466 - acc: 0.798 - ETA: 2s - loss: 0.5454 - acc: 0.798 - ETA: 1s - loss: 0.5460 - acc: 0.797 - ETA: 1s - loss: 0.5469 - acc: 0.796 - ETA: 0s - loss: 0.5463 - acc: 0.796 - ETA: 0s - loss: 0.5449 - acc: 0.796 - ETA: 0s - loss: 0.5432 - acc: 0.797 - 58s 22ms/step - loss: 0.5430 - acc: 0.7976 - val_loss: 1.1397 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.01071\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624/2624 [==============================] - ETA: 49s - loss: 0.6322 - acc: 0.80 - ETA: 50s - loss: 0.6338 - acc: 0.80 - ETA: 50s - loss: 0.6881 - acc: 0.78 - ETA: 49s - loss: 0.6145 - acc: 0.78 - ETA: 49s - loss: 0.5679 - acc: 0.81 - ETA: 48s - loss: 0.5310 - acc: 0.82 - ETA: 48s - loss: 0.4783 - acc: 0.84 - ETA: 48s - loss: 0.4664 - acc: 0.85 - ETA: 47s - loss: 0.4684 - acc: 0.83 - ETA: 47s - loss: 0.4939 - acc: 0.83 - ETA: 47s - loss: 0.4842 - acc: 0.83 - ETA: 46s - loss: 0.4842 - acc: 0.83 - ETA: 46s - loss: 0.4748 - acc: 0.84 - ETA: 46s - loss: 0.4697 - acc: 0.84 - ETA: 45s - loss: 0.4622 - acc: 0.84 - ETA: 45s - loss: 0.4662 - acc: 0.83 - ETA: 44s - loss: 0.4793 - acc: 0.82 - ETA: 44s - loss: 0.4884 - acc: 0.81 - ETA: 44s - loss: 0.4864 - acc: 0.82 - ETA: 43s - loss: 0.4825 - acc: 0.82 - ETA: 43s - loss: 0.5017 - acc: 0.81 - ETA: 42s - loss: 0.4919 - acc: 0.82 - ETA: 42s - loss: 0.5030 - acc: 0.81 - ETA: 42s - loss: 0.5140 - acc: 0.81 - ETA: 41s - loss: 0.5148 - acc: 0.81 - ETA: 41s - loss: 0.5118 - acc: 0.81 - ETA: 40s - loss: 0.5196 - acc: 0.81 - ETA: 40s - loss: 0.5197 - acc: 0.80 - ETA: 40s - loss: 0.5248 - acc: 0.80 - ETA: 39s - loss: 0.5303 - acc: 0.80 - ETA: 39s - loss: 0.5279 - acc: 0.80 - ETA: 38s - loss: 0.5273 - acc: 0.80 - ETA: 38s - loss: 0.5224 - acc: 0.80 - ETA: 38s - loss: 0.5201 - acc: 0.80 - ETA: 37s - loss: 0.5364 - acc: 0.79 - ETA: 37s - loss: 0.5391 - acc: 0.79 - ETA: 37s - loss: 0.5376 - acc: 0.80 - ETA: 36s - loss: 0.5298 - acc: 0.80 - ETA: 36s - loss: 0.5242 - acc: 0.80 - ETA: 35s - loss: 0.5219 - acc: 0.80 - ETA: 35s - loss: 0.5159 - acc: 0.81 - ETA: 35s - loss: 0.5190 - acc: 0.81 - ETA: 34s - loss: 0.5111 - acc: 0.81 - ETA: 34s - loss: 0.5152 - acc: 0.81 - ETA: 33s - loss: 0.5080 - acc: 0.81 - ETA: 33s - loss: 0.5053 - acc: 0.81 - ETA: 33s - loss: 0.5105 - acc: 0.81 - ETA: 32s - loss: 0.5116 - acc: 0.81 - ETA: 32s - loss: 0.5150 - acc: 0.81 - ETA: 31s - loss: 0.5177 - acc: 0.81 - ETA: 31s - loss: 0.5159 - acc: 0.81 - ETA: 31s - loss: 0.5177 - acc: 0.80 - ETA: 30s - loss: 0.5181 - acc: 0.81 - ETA: 30s - loss: 0.5160 - acc: 0.81 - ETA: 29s - loss: 0.5133 - acc: 0.81 - ETA: 29s - loss: 0.5081 - acc: 0.81 - ETA: 29s - loss: 0.5037 - acc: 0.81 - ETA: 28s - loss: 0.4985 - acc: 0.81 - ETA: 28s - loss: 0.4973 - acc: 0.81 - ETA: 28s - loss: 0.5002 - acc: 0.81 - ETA: 27s - loss: 0.4980 - acc: 0.81 - ETA: 27s - loss: 0.4962 - acc: 0.81 - ETA: 26s - loss: 0.4926 - acc: 0.81 - ETA: 26s - loss: 0.4884 - acc: 0.82 - ETA: 26s - loss: 0.4826 - acc: 0.82 - ETA: 25s - loss: 0.4847 - acc: 0.82 - ETA: 25s - loss: 0.4849 - acc: 0.82 - ETA: 24s - loss: 0.4856 - acc: 0.82 - ETA: 24s - loss: 0.4899 - acc: 0.81 - ETA: 24s - loss: 0.4926 - acc: 0.81 - ETA: 23s - loss: 0.4926 - acc: 0.81 - ETA: 23s - loss: 0.4946 - acc: 0.81 - ETA: 22s - loss: 0.4949 - acc: 0.81 - ETA: 22s - loss: 0.4929 - acc: 0.81 - ETA: 22s - loss: 0.4889 - acc: 0.82 - ETA: 21s - loss: 0.4898 - acc: 0.82 - ETA: 21s - loss: 0.4870 - acc: 0.82 - ETA: 20s - loss: 0.4880 - acc: 0.82 - ETA: 20s - loss: 0.4895 - acc: 0.82 - ETA: 20s - loss: 0.4907 - acc: 0.82 - ETA: 19s - loss: 0.4931 - acc: 0.81 - ETA: 19s - loss: 0.4914 - acc: 0.81 - ETA: 18s - loss: 0.4908 - acc: 0.81 - ETA: 18s - loss: 0.4938 - acc: 0.81 - ETA: 18s - loss: 0.4946 - acc: 0.81 - ETA: 17s - loss: 0.4923 - acc: 0.81 - ETA: 17s - loss: 0.4910 - acc: 0.81 - ETA: 17s - loss: 0.4929 - acc: 0.81 - ETA: 16s - loss: 0.4897 - acc: 0.81 - ETA: 16s - loss: 0.4878 - acc: 0.82 - ETA: 15s - loss: 0.4858 - acc: 0.82 - ETA: 15s - loss: 0.4887 - acc: 0.81 - ETA: 15s - loss: 0.4857 - acc: 0.81 - ETA: 14s - loss: 0.4844 - acc: 0.82 - ETA: 14s - loss: 0.4843 - acc: 0.82 - ETA: 13s - loss: 0.4811 - acc: 0.82 - ETA: 13s - loss: 0.4809 - acc: 0.82 - ETA: 13s - loss: 0.4869 - acc: 0.81 - ETA: 12s - loss: 0.4926 - acc: 0.81 - ETA: 12s - loss: 0.4919 - acc: 0.81 - ETA: 11s - loss: 0.4907 - acc: 0.81 - ETA: 11s - loss: 0.4914 - acc: 0.81 - ETA: 11s - loss: 0.4909 - acc: 0.81 - ETA: 10s - loss: 0.4885 - acc: 0.81 - ETA: 10s - loss: 0.4872 - acc: 0.81 - ETA: 9s - loss: 0.4869 - acc: 0.8198 - ETA: 9s - loss: 0.4865 - acc: 0.820 - ETA: 9s - loss: 0.4851 - acc: 0.820 - ETA: 8s - loss: 0.4818 - acc: 0.822 - ETA: 8s - loss: 0.4815 - acc: 0.822 - ETA: 7s - loss: 0.4802 - acc: 0.823 - ETA: 7s - loss: 0.4798 - acc: 0.823 - ETA: 7s - loss: 0.4787 - acc: 0.823 - ETA: 6s - loss: 0.4770 - acc: 0.824 - ETA: 6s - loss: 0.4753 - acc: 0.825 - ETA: 5s - loss: 0.4735 - acc: 0.826 - ETA: 5s - loss: 0.4712 - acc: 0.827 - ETA: 5s - loss: 0.4701 - acc: 0.827 - ETA: 4s - loss: 0.4681 - acc: 0.828 - ETA: 4s - loss: 0.4665 - acc: 0.829 - ETA: 4s - loss: 0.4671 - acc: 0.829 - ETA: 3s - loss: 0.4665 - acc: 0.829 - ETA: 3s - loss: 0.4652 - acc: 0.830 - ETA: 2s - loss: 0.4642 - acc: 0.831 - ETA: 2s - loss: 0.4645 - acc: 0.831 - ETA: 2s - loss: 0.4664 - acc: 0.831 - ETA: 1s - loss: 0.4670 - acc: 0.830 - ETA: 1s - loss: 0.4678 - acc: 0.830 - ETA: 0s - loss: 0.4668 - acc: 0.829 - ETA: 0s - loss: 0.4659 - acc: 0.829 - ETA: 0s - loss: 0.4653 - acc: 0.829 - 59s 22ms/step - loss: 0.4670 - acc: 0.8289 - val_loss: 1.1442 - val_acc: 0.5618\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.01071\n",
      "Epoch 8/8\n",
      "2624/2624 [==============================] - ETA: 52s - loss: 1.2292 - acc: 0.60 - ETA: 51s - loss: 0.7776 - acc: 0.75 - ETA: 50s - loss: 0.6492 - acc: 0.75 - ETA: 50s - loss: 0.5798 - acc: 0.80 - ETA: 49s - loss: 0.5013 - acc: 0.83 - ETA: 49s - loss: 0.4383 - acc: 0.85 - ETA: 49s - loss: 0.4109 - acc: 0.85 - ETA: 48s - loss: 0.3972 - acc: 0.85 - ETA: 48s - loss: 0.4011 - acc: 0.86 - ETA: 47s - loss: 0.3996 - acc: 0.86 - ETA: 47s - loss: 0.3826 - acc: 0.86 - ETA: 47s - loss: 0.3605 - acc: 0.87 - ETA: 46s - loss: 0.3523 - acc: 0.88 - ETA: 46s - loss: 0.3667 - acc: 0.87 - ETA: 45s - loss: 0.3964 - acc: 0.87 - ETA: 45s - loss: 0.3915 - acc: 0.87 - ETA: 45s - loss: 0.3894 - acc: 0.87 - ETA: 44s - loss: 0.3981 - acc: 0.86 - ETA: 44s - loss: 0.4063 - acc: 0.86 - ETA: 43s - loss: 0.4151 - acc: 0.86 - ETA: 43s - loss: 0.3993 - acc: 0.86 - ETA: 43s - loss: 0.3924 - acc: 0.87 - ETA: 42s - loss: 0.3872 - acc: 0.87 - ETA: 42s - loss: 0.3745 - acc: 0.87 - ETA: 41s - loss: 0.3779 - acc: 0.87 - ETA: 41s - loss: 0.3760 - acc: 0.87 - ETA: 41s - loss: 0.3751 - acc: 0.87 - ETA: 40s - loss: 0.3777 - acc: 0.87 - ETA: 40s - loss: 0.3785 - acc: 0.87 - ETA: 39s - loss: 0.3873 - acc: 0.87 - ETA: 39s - loss: 0.3849 - acc: 0.86 - ETA: 39s - loss: 0.3821 - acc: 0.86 - ETA: 38s - loss: 0.3779 - acc: 0.86 - ETA: 38s - loss: 0.3795 - acc: 0.86 - ETA: 37s - loss: 0.3876 - acc: 0.86 - ETA: 37s - loss: 0.3821 - acc: 0.86 - ETA: 37s - loss: 0.3798 - acc: 0.86 - ETA: 36s - loss: 0.3844 - acc: 0.86 - ETA: 36s - loss: 0.3902 - acc: 0.85 - ETA: 35s - loss: 0.3899 - acc: 0.86 - ETA: 35s - loss: 0.3870 - acc: 0.86 - ETA: 35s - loss: 0.3925 - acc: 0.85 - ETA: 34s - loss: 0.4052 - acc: 0.85 - ETA: 34s - loss: 0.4067 - acc: 0.85 - ETA: 33s - loss: 0.4033 - acc: 0.85 - ETA: 33s - loss: 0.4009 - acc: 0.85 - ETA: 33s - loss: 0.3996 - acc: 0.85 - ETA: 32s - loss: 0.4016 - acc: 0.85 - ETA: 32s - loss: 0.4083 - acc: 0.85 - ETA: 32s - loss: 0.4124 - acc: 0.85 - ETA: 31s - loss: 0.4068 - acc: 0.85 - ETA: 31s - loss: 0.4077 - acc: 0.85 - ETA: 30s - loss: 0.4017 - acc: 0.85 - ETA: 30s - loss: 0.4025 - acc: 0.85 - ETA: 30s - loss: 0.3995 - acc: 0.85 - ETA: 29s - loss: 0.3961 - acc: 0.85 - ETA: 29s - loss: 0.3948 - acc: 0.85 - ETA: 28s - loss: 0.3954 - acc: 0.85 - ETA: 28s - loss: 0.3955 - acc: 0.85 - ETA: 28s - loss: 0.3919 - acc: 0.85 - ETA: 27s - loss: 0.3913 - acc: 0.85 - ETA: 27s - loss: 0.3909 - acc: 0.85 - ETA: 26s - loss: 0.3928 - acc: 0.86 - ETA: 26s - loss: 0.3904 - acc: 0.86 - ETA: 26s - loss: 0.3894 - acc: 0.86 - ETA: 25s - loss: 0.3874 - acc: 0.86 - ETA: 25s - loss: 0.3882 - acc: 0.86 - ETA: 24s - loss: 0.3858 - acc: 0.86 - ETA: 24s - loss: 0.3863 - acc: 0.86 - ETA: 24s - loss: 0.3863 - acc: 0.86 - ETA: 23s - loss: 0.3835 - acc: 0.86 - ETA: 23s - loss: 0.3825 - acc: 0.86 - ETA: 23s - loss: 0.3789 - acc: 0.86 - ETA: 22s - loss: 0.3774 - acc: 0.86 - ETA: 22s - loss: 0.3750 - acc: 0.86 - ETA: 21s - loss: 0.3748 - acc: 0.86 - ETA: 21s - loss: 0.3775 - acc: 0.86 - ETA: 21s - loss: 0.3776 - acc: 0.86 - ETA: 20s - loss: 0.3784 - acc: 0.86 - ETA: 20s - loss: 0.3794 - acc: 0.86 - ETA: 19s - loss: 0.3788 - acc: 0.86 - ETA: 19s - loss: 0.3809 - acc: 0.86 - ETA: 19s - loss: 0.3819 - acc: 0.86 - ETA: 18s - loss: 0.3813 - acc: 0.86 - ETA: 18s - loss: 0.3819 - acc: 0.86 - ETA: 17s - loss: 0.3856 - acc: 0.85 - ETA: 17s - loss: 0.3901 - acc: 0.85 - ETA: 17s - loss: 0.3886 - acc: 0.85 - ETA: 16s - loss: 0.3877 - acc: 0.85 - ETA: 16s - loss: 0.3856 - acc: 0.85 - ETA: 15s - loss: 0.3874 - acc: 0.85 - ETA: 15s - loss: 0.3887 - acc: 0.85 - ETA: 15s - loss: 0.3872 - acc: 0.85 - ETA: 14s - loss: 0.3912 - acc: 0.85 - ETA: 14s - loss: 0.3914 - acc: 0.85 - ETA: 13s - loss: 0.3896 - acc: 0.85 - ETA: 13s - loss: 0.3892 - acc: 0.85 - ETA: 13s - loss: 0.3872 - acc: 0.85 - ETA: 12s - loss: 0.3884 - acc: 0.85 - ETA: 12s - loss: 0.3864 - acc: 0.86 - ETA: 11s - loss: 0.3865 - acc: 0.86 - ETA: 11s - loss: 0.3880 - acc: 0.85 - ETA: 11s - loss: 0.3900 - acc: 0.85 - ETA: 10s - loss: 0.3885 - acc: 0.85 - ETA: 10s - loss: 0.3898 - acc: 0.85 - ETA: 9s - loss: 0.3891 - acc: 0.8604 - ETA: 9s - loss: 0.3876 - acc: 0.860 - ETA: 9s - loss: 0.3853 - acc: 0.861 - ETA: 8s - loss: 0.3836 - acc: 0.861 - ETA: 8s - loss: 0.3810 - acc: 0.863 - ETA: 7s - loss: 0.3798 - acc: 0.864 - ETA: 7s - loss: 0.3792 - acc: 0.864 - ETA: 7s - loss: 0.3790 - acc: 0.863 - ETA: 6s - loss: 0.3772 - acc: 0.864 - ETA: 6s - loss: 0.3763 - acc: 0.865 - ETA: 6s - loss: 0.3811 - acc: 0.864 - ETA: 5s - loss: 0.3832 - acc: 0.863 - ETA: 5s - loss: 0.3829 - acc: 0.863 - ETA: 4s - loss: 0.3843 - acc: 0.861 - ETA: 4s - loss: 0.3834 - acc: 0.861 - ETA: 4s - loss: 0.3855 - acc: 0.860 - ETA: 3s - loss: 0.3838 - acc: 0.861 - ETA: 3s - loss: 0.3825 - acc: 0.862 - ETA: 2s - loss: 0.3815 - acc: 0.862 - ETA: 2s - loss: 0.3812 - acc: 0.862 - ETA: 2s - loss: 0.3799 - acc: 0.863 - ETA: 1s - loss: 0.3775 - acc: 0.864 - ETA: 1s - loss: 0.3780 - acc: 0.864 - ETA: 0s - loss: 0.3777 - acc: 0.864 - ETA: 0s - loss: 0.3762 - acc: 0.865 - ETA: 0s - loss: 0.3764 - acc: 0.865 - 59s 22ms/step - loss: 0.3759 - acc: 0.8655 - val_loss: 0.9203 - val_acc: 0.6834\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.01071 to 0.92031, saving model to saved_models/weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25940394f60>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 8\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 77.9000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted diagnosis for each image in test set\n",
    "diagnosis_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(diagnosis_predictions)==np.argmax(test_targets, axis=1))/len(diagnosis_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step N: Create a CNN to Classify Diagnosis (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/Resnet50Data.npz')\n",
    "train_transfer_learning_model_features = bottleneck_features['train']\n",
    "valid_transfer_learning_model_features = bottleneck_features['valid']\n",
    "test_transfer_learning_model_features = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_51 (Conv2D)           (None, 1, 1, 75)          614475    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 1, 1, 75)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 1, 1, 100)         30100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 1, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 1, 1, 125)         50125     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling (None, 1, 1, 125)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1, 1, 125)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 504       \n",
      "=================================================================\n",
      "Total params: 695,204\n",
      "Trainable params: 695,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_learning_model = Sequential()\n",
    "transfer_learning_model.add(Conv2D(filters=75, kernel_size=2, padding='same', activation='relu', \n",
    "                                   input_shape=train_transfer_learning_model_features.shape[1:]))\n",
    "transfer_learning_model.add(MaxPooling2D(pool_size=1))\n",
    "transfer_learning_model.add(Conv2D(filters=100, kernel_size=2, padding='same', activation='relu'))\n",
    "transfer_learning_model.add(MaxPooling2D(pool_size=1))\n",
    "transfer_learning_model.add(Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'))\n",
    "transfer_learning_model.add(MaxPooling2D(pool_size=1))\n",
    "transfer_learning_model.add(Dropout(0.3))\n",
    "transfer_learning_model.add(Flatten())\n",
    "transfer_learning_model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "transfer_learning_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_model.compile(loss='categorical_crossentropy',\n",
    "                                optimizer='rmsprop',\n",
    "                                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624\n",
      "2624\n",
      "835\n",
      "938\n",
      "Train on 2624 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "2624/2624 [==============================] - ETA: 1:08 - loss: 1.3495 - acc: 0.500 - ETA: 14s - loss: 1.5084 - acc: 0.280 - ETA: 7s - loss: 1.4485 - acc: 0.2318 - ETA: 5s - loss: 1.4342 - acc: 0.236 - ETA: 4s - loss: 1.4207 - acc: 0.239 - ETA: 3s - loss: 1.4151 - acc: 0.232 - ETA: 3s - loss: 1.4145 - acc: 0.238 - ETA: 2s - loss: 1.4102 - acc: 0.250 - ETA: 2s - loss: 1.4102 - acc: 0.244 - ETA: 2s - loss: 1.4074 - acc: 0.244 - ETA: 2s - loss: 1.4069 - acc: 0.246 - ETA: 2s - loss: 1.4044 - acc: 0.247 - ETA: 1s - loss: 1.4033 - acc: 0.252 - ETA: 1s - loss: 1.4039 - acc: 0.251 - ETA: 1s - loss: 1.4028 - acc: 0.252 - ETA: 1s - loss: 1.4028 - acc: 0.250 - ETA: 1s - loss: 1.4029 - acc: 0.246 - ETA: 1s - loss: 1.4021 - acc: 0.244 - ETA: 1s - loss: 1.4027 - acc: 0.240 - ETA: 0s - loss: 1.4018 - acc: 0.241 - ETA: 0s - loss: 1.4015 - acc: 0.240 - ETA: 0s - loss: 1.4010 - acc: 0.238 - ETA: 0s - loss: 1.3997 - acc: 0.241 - ETA: 0s - loss: 1.3992 - acc: 0.244 - ETA: 0s - loss: 1.3989 - acc: 0.244 - ETA: 0s - loss: 1.3993 - acc: 0.242 - ETA: 0s - loss: 1.3988 - acc: 0.242 - ETA: 0s - loss: 1.3993 - acc: 0.242 - ETA: 0s - loss: 1.3988 - acc: 0.243 - ETA: 0s - loss: 1.3987 - acc: 0.245 - ETA: 0s - loss: 1.3987 - acc: 0.245 - ETA: 0s - loss: 1.3982 - acc: 0.246 - 2s 945us/step - loss: 1.3982 - acc: 0.2470 - val_loss: 1.4120 - val_acc: 0.2371\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.41203, saving model to saved_models/weights.best.transfer_learning_model.hdf5\n",
      "Epoch 2/10\n",
      "2624/2624 [==============================] - ETA: 1s - loss: 1.3980 - acc: 0.350 - ETA: 1s - loss: 1.3830 - acc: 0.320 - ETA: 1s - loss: 1.3913 - acc: 0.261 - ETA: 1s - loss: 1.3927 - acc: 0.284 - ETA: 1s - loss: 1.3914 - acc: 0.264 - ETA: 1s - loss: 1.3949 - acc: 0.257 - ETA: 1s - loss: 1.3908 - acc: 0.259 - ETA: 1s - loss: 1.3905 - acc: 0.265 - ETA: 1s - loss: 1.3921 - acc: 0.258 - ETA: 1s - loss: 1.3914 - acc: 0.260 - ETA: 1s - loss: 1.3894 - acc: 0.270 - ETA: 1s - loss: 1.3909 - acc: 0.273 - ETA: 1s - loss: 1.3880 - acc: 0.273 - ETA: 1s - loss: 1.3891 - acc: 0.270 - ETA: 0s - loss: 1.3885 - acc: 0.274 - ETA: 0s - loss: 1.3892 - acc: 0.278 - ETA: 0s - loss: 1.3897 - acc: 0.273 - ETA: 0s - loss: 1.3894 - acc: 0.275 - ETA: 0s - loss: 1.3881 - acc: 0.279 - ETA: 0s - loss: 1.3882 - acc: 0.281 - ETA: 0s - loss: 1.3884 - acc: 0.281 - ETA: 0s - loss: 1.3883 - acc: 0.279 - ETA: 0s - loss: 1.3871 - acc: 0.282 - ETA: 0s - loss: 1.3884 - acc: 0.278 - ETA: 0s - loss: 1.3879 - acc: 0.280 - ETA: 0s - loss: 1.3876 - acc: 0.283 - ETA: 0s - loss: 1.3882 - acc: 0.282 - ETA: 0s - loss: 1.3886 - acc: 0.281 - ETA: 0s - loss: 1.3886 - acc: 0.279 - ETA: 0s - loss: 1.3882 - acc: 0.280 - ETA: 0s - loss: 1.3881 - acc: 0.281 - ETA: 0s - loss: 1.3885 - acc: 0.279 - 2s 746us/step - loss: 1.3890 - acc: 0.2778 - val_loss: 1.3871 - val_acc: 0.2407\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.41203 to 1.38710, saving model to saved_models/weights.best.transfer_learning_model.hdf5\n",
      "Epoch 3/10\n",
      "2624/2624 [==============================] - ETA: 1s - loss: 1.3831 - acc: 0.300 - ETA: 1s - loss: 1.3741 - acc: 0.350 - ETA: 1s - loss: 1.3786 - acc: 0.311 - ETA: 1s - loss: 1.3753 - acc: 0.288 - ETA: 1s - loss: 1.3828 - acc: 0.284 - ETA: 1s - loss: 1.3826 - acc: 0.278 - ETA: 1s - loss: 1.3803 - acc: 0.281 - ETA: 1s - loss: 1.3822 - acc: 0.278 - ETA: 1s - loss: 1.3810 - acc: 0.279 - ETA: 1s - loss: 1.3810 - acc: 0.284 - ETA: 1s - loss: 1.3767 - acc: 0.297 - ETA: 1s - loss: 1.3756 - acc: 0.296 - ETA: 1s - loss: 1.3743 - acc: 0.294 - ETA: 1s - loss: 1.3731 - acc: 0.294 - ETA: 1s - loss: 1.3756 - acc: 0.292 - ETA: 0s - loss: 1.3752 - acc: 0.292 - ETA: 0s - loss: 1.3766 - acc: 0.290 - ETA: 0s - loss: 1.3785 - acc: 0.284 - ETA: 0s - loss: 1.3784 - acc: 0.282 - ETA: 0s - loss: 1.3780 - acc: 0.287 - ETA: 0s - loss: 1.3787 - acc: 0.283 - ETA: 0s - loss: 1.3792 - acc: 0.283 - ETA: 0s - loss: 1.3802 - acc: 0.282 - ETA: 0s - loss: 1.3809 - acc: 0.280 - ETA: 0s - loss: 1.3812 - acc: 0.279 - ETA: 0s - loss: 1.3805 - acc: 0.282 - ETA: 0s - loss: 1.3805 - acc: 0.283 - ETA: 0s - loss: 1.3806 - acc: 0.281 - ETA: 0s - loss: 1.3808 - acc: 0.279 - 2s 699us/step - loss: 1.3805 - acc: 0.2797 - val_loss: 1.4156 - val_acc: 0.2096\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.38710\n",
      "Epoch 4/10\n",
      "2624/2624 [==============================] - ETA: 1s - loss: 1.4029 - acc: 0.150 - ETA: 1s - loss: 1.3684 - acc: 0.325 - ETA: 1s - loss: 1.3562 - acc: 0.331 - ETA: 1s - loss: 1.3540 - acc: 0.334 - ETA: 1s - loss: 1.3563 - acc: 0.333 - ETA: 1s - loss: 1.3645 - acc: 0.323 - ETA: 1s - loss: 1.3653 - acc: 0.322 - ETA: 1s - loss: 1.3618 - acc: 0.325 - ETA: 1s - loss: 1.3614 - acc: 0.314 - ETA: 0s - loss: 1.3622 - acc: 0.313 - ETA: 0s - loss: 1.3657 - acc: 0.314 - ETA: 0s - loss: 1.3632 - acc: 0.316 - ETA: 0s - loss: 1.3633 - acc: 0.312 - ETA: 0s - loss: 1.3651 - acc: 0.311 - ETA: 0s - loss: 1.3640 - acc: 0.314 - ETA: 0s - loss: 1.3646 - acc: 0.315 - ETA: 0s - loss: 1.3664 - acc: 0.312 - ETA: 0s - loss: 1.3661 - acc: 0.313 - ETA: 0s - loss: 1.3669 - acc: 0.310 - ETA: 0s - loss: 1.3666 - acc: 0.311 - ETA: 0s - loss: 1.3652 - acc: 0.312 - ETA: 0s - loss: 1.3657 - acc: 0.312 - ETA: 0s - loss: 1.3664 - acc: 0.314 - ETA: 0s - loss: 1.3663 - acc: 0.312 - ETA: 0s - loss: 1.3660 - acc: 0.313 - ETA: 0s - loss: 1.3668 - acc: 0.312 - 2s 635us/step - loss: 1.3668 - acc: 0.3121 - val_loss: 1.4034 - val_acc: 0.2383\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.38710\n",
      "Epoch 5/10\n",
      "2624/2624 [==============================] - ETA: 1s - loss: 1.3043 - acc: 0.350 - ETA: 1s - loss: 1.3411 - acc: 0.330 - ETA: 1s - loss: 1.3217 - acc: 0.390 - ETA: 1s - loss: 1.3143 - acc: 0.383 - ETA: 1s - loss: 1.3206 - acc: 0.383 - ETA: 1s - loss: 1.3221 - acc: 0.375 - ETA: 1s - loss: 1.3237 - acc: 0.371 - ETA: 1s - loss: 1.3266 - acc: 0.368 - ETA: 1s - loss: 1.3270 - acc: 0.362 - ETA: 0s - loss: 1.3267 - acc: 0.359 - ETA: 0s - loss: 1.3186 - acc: 0.365 - ETA: 0s - loss: 1.3241 - acc: 0.358 - ETA: 0s - loss: 1.3274 - acc: 0.352 - ETA: 0s - loss: 1.3322 - acc: 0.346 - ETA: 0s - loss: 1.3301 - acc: 0.351 - ETA: 0s - loss: 1.3314 - acc: 0.352 - ETA: 0s - loss: 1.3297 - acc: 0.352 - ETA: 0s - loss: 1.3283 - acc: 0.350 - ETA: 0s - loss: 1.3292 - acc: 0.351 - ETA: 0s - loss: 1.3275 - acc: 0.355 - ETA: 0s - loss: 1.3293 - acc: 0.358 - ETA: 0s - loss: 1.3308 - acc: 0.357 - ETA: 0s - loss: 1.3321 - acc: 0.357 - ETA: 0s - loss: 1.3321 - acc: 0.356 - ETA: 0s - loss: 1.3334 - acc: 0.355 - ETA: 0s - loss: 1.3341 - acc: 0.356 - ETA: 0s - loss: 1.3356 - acc: 0.354 - 2s 642us/step - loss: 1.3363 - acc: 0.3537 - val_loss: 1.4006 - val_acc: 0.2754\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.38710\n",
      "Epoch 6/10\n",
      "2624/2624 [==============================] - ETA: 1s - loss: 1.2410 - acc: 0.600 - ETA: 1s - loss: 1.2882 - acc: 0.441 - ETA: 1s - loss: 1.3069 - acc: 0.409 - ETA: 1s - loss: 1.3118 - acc: 0.409 - ETA: 1s - loss: 1.3121 - acc: 0.407 - ETA: 1s - loss: 1.3017 - acc: 0.412 - ETA: 1s - loss: 1.2978 - acc: 0.416 - ETA: 1s - loss: 1.2919 - acc: 0.409 - ETA: 1s - loss: 1.2882 - acc: 0.406 - ETA: 1s - loss: 1.2962 - acc: 0.402 - ETA: 0s - loss: 1.2928 - acc: 0.397 - ETA: 0s - loss: 1.2912 - acc: 0.400 - ETA: 0s - loss: 1.2889 - acc: 0.400 - ETA: 0s - loss: 1.2900 - acc: 0.399 - ETA: 0s - loss: 1.2900 - acc: 0.395 - ETA: 0s - loss: 1.2936 - acc: 0.392 - ETA: 0s - loss: 1.2916 - acc: 0.393 - ETA: 0s - loss: 1.2959 - acc: 0.386 - ETA: 0s - loss: 1.2914 - acc: 0.390 - ETA: 0s - loss: 1.2911 - acc: 0.391 - ETA: 0s - loss: 1.2917 - acc: 0.391 - ETA: 0s - loss: 1.2934 - acc: 0.390 - ETA: 0s - loss: 1.2922 - acc: 0.390 - ETA: 0s - loss: 1.2907 - acc: 0.392 - ETA: 0s - loss: 1.2928 - acc: 0.388 - ETA: 0s - loss: 1.2943 - acc: 0.388 - ETA: 0s - loss: 1.2931 - acc: 0.390 - ETA: 0s - loss: 1.2925 - acc: 0.391 - ETA: 0s - loss: 1.2935 - acc: 0.388 - 2s 685us/step - loss: 1.2936 - acc: 0.3883 - val_loss: 1.4825 - val_acc: 0.2635\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.38710\n",
      "Epoch 7/10\n",
      "2624/2624 [==============================] - ETA: 1s - loss: 1.2777 - acc: 0.400 - ETA: 1s - loss: 1.2522 - acc: 0.440 - ETA: 1s - loss: 1.2187 - acc: 0.444 - ETA: 1s - loss: 1.2395 - acc: 0.438 - ETA: 1s - loss: 1.2448 - acc: 0.427 - ETA: 1s - loss: 1.2401 - acc: 0.432 - ETA: 1s - loss: 1.2382 - acc: 0.432 - ETA: 1s - loss: 1.2288 - acc: 0.436 - ETA: 1s - loss: 1.2345 - acc: 0.434 - ETA: 1s - loss: 1.2384 - acc: 0.425 - ETA: 0s - loss: 1.2353 - acc: 0.430 - ETA: 0s - loss: 1.2356 - acc: 0.427 - ETA: 0s - loss: 1.2384 - acc: 0.431 - ETA: 0s - loss: 1.2362 - acc: 0.432 - ETA: 0s - loss: 1.2240 - acc: 0.443 - ETA: 0s - loss: 1.2302 - acc: 0.438 - ETA: 0s - loss: 1.2358 - acc: 0.437 - ETA: 0s - loss: 1.2380 - acc: 0.434 - ETA: 0s - loss: 1.2453 - acc: 0.428 - ETA: 0s - loss: 1.2449 - acc: 0.430 - ETA: 0s - loss: 1.2476 - acc: 0.426 - ETA: 0s - loss: 1.2477 - acc: 0.429 - ETA: 0s - loss: 1.2434 - acc: 0.434 - ETA: 0s - loss: 1.2473 - acc: 0.432 - ETA: 0s - loss: 1.2477 - acc: 0.431 - ETA: 0s - loss: 1.2465 - acc: 0.431 - ETA: 0s - loss: 1.2435 - acc: 0.432 - ETA: 0s - loss: 1.2434 - acc: 0.430 - 2s 675us/step - loss: 1.2429 - acc: 0.4310 - val_loss: 1.5260 - val_acc: 0.2922\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.38710\n",
      "Epoch 8/10\n",
      "2624/2624 [==============================] - ETA: 1s - loss: 1.2028 - acc: 0.500 - ETA: 1s - loss: 1.1847 - acc: 0.466 - ETA: 1s - loss: 1.1441 - acc: 0.495 - ETA: 1s - loss: 1.1419 - acc: 0.490 - ETA: 1s - loss: 1.1287 - acc: 0.502 - ETA: 1s - loss: 1.1556 - acc: 0.487 - ETA: 1s - loss: 1.1509 - acc: 0.491 - ETA: 1s - loss: 1.1605 - acc: 0.485 - ETA: 1s - loss: 1.1587 - acc: 0.486 - ETA: 1s - loss: 1.1571 - acc: 0.486 - ETA: 0s - loss: 1.1509 - acc: 0.492 - ETA: 0s - loss: 1.1511 - acc: 0.495 - ETA: 0s - loss: 1.1564 - acc: 0.489 - ETA: 0s - loss: 1.1610 - acc: 0.484 - ETA: 0s - loss: 1.1558 - acc: 0.487 - ETA: 0s - loss: 1.1581 - acc: 0.487 - ETA: 0s - loss: 1.1607 - acc: 0.484 - ETA: 0s - loss: 1.1640 - acc: 0.483 - ETA: 0s - loss: 1.1634 - acc: 0.483 - ETA: 0s - loss: 1.1615 - acc: 0.483 - ETA: 0s - loss: 1.1749 - acc: 0.475 - ETA: 0s - loss: 1.1818 - acc: 0.469 - ETA: 0s - loss: 1.1831 - acc: 0.471 - ETA: 0s - loss: 1.1805 - acc: 0.471 - ETA: 0s - loss: 1.1803 - acc: 0.471 - ETA: 0s - loss: 1.1807 - acc: 0.473 - 2s 634us/step - loss: 1.1820 - acc: 0.4722 - val_loss: 1.5687 - val_acc: 0.2515\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.38710\n",
      "Epoch 9/10\n",
      "2624/2624 [==============================] - ETA: 2s - loss: 1.2296 - acc: 0.450 - ETA: 1s - loss: 1.1099 - acc: 0.507 - ETA: 1s - loss: 1.1233 - acc: 0.476 - ETA: 1s - loss: 1.0992 - acc: 0.497 - ETA: 1s - loss: 1.0847 - acc: 0.514 - ETA: 1s - loss: 1.0891 - acc: 0.518 - ETA: 1s - loss: 1.0803 - acc: 0.527 - ETA: 0s - loss: 1.0783 - acc: 0.531 - ETA: 0s - loss: 1.0724 - acc: 0.534 - ETA: 0s - loss: 1.0825 - acc: 0.527 - ETA: 0s - loss: 1.0744 - acc: 0.530 - ETA: 0s - loss: 1.0677 - acc: 0.536 - ETA: 0s - loss: 1.0760 - acc: 0.534 - ETA: 0s - loss: 1.0842 - acc: 0.527 - ETA: 0s - loss: 1.0899 - acc: 0.525 - ETA: 0s - loss: 1.0905 - acc: 0.525 - ETA: 0s - loss: 1.0852 - acc: 0.529 - ETA: 0s - loss: 1.0944 - acc: 0.526 - ETA: 0s - loss: 1.0899 - acc: 0.530 - ETA: 0s - loss: 1.0889 - acc: 0.534 - ETA: 0s - loss: 1.0940 - acc: 0.530 - ETA: 0s - loss: 1.0978 - acc: 0.528 - ETA: 0s - loss: 1.0957 - acc: 0.529 - 2s 601us/step - loss: 1.0989 - acc: 0.5271 - val_loss: 1.7740 - val_acc: 0.2491\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.38710\n",
      "Epoch 10/10\n",
      "2624/2624 [==============================] - ETA: 0s - loss: 1.2081 - acc: 0.500 - ETA: 1s - loss: 0.9607 - acc: 0.620 - ETA: 1s - loss: 0.9537 - acc: 0.633 - ETA: 1s - loss: 0.9741 - acc: 0.615 - ETA: 1s - loss: 0.9532 - acc: 0.611 - ETA: 1s - loss: 0.9588 - acc: 0.605 - ETA: 1s - loss: 0.9835 - acc: 0.592 - ETA: 1s - loss: 1.0044 - acc: 0.580 - ETA: 0s - loss: 1.0028 - acc: 0.583 - ETA: 0s - loss: 1.0045 - acc: 0.578 - ETA: 0s - loss: 1.0055 - acc: 0.579 - ETA: 0s - loss: 1.0166 - acc: 0.575 - ETA: 0s - loss: 1.0097 - acc: 0.574 - ETA: 0s - loss: 1.0010 - acc: 0.578 - ETA: 0s - loss: 1.0032 - acc: 0.577 - ETA: 0s - loss: 1.0114 - acc: 0.576 - ETA: 0s - loss: 1.0131 - acc: 0.574 - ETA: 0s - loss: 1.0092 - acc: 0.579 - ETA: 0s - loss: 1.0144 - acc: 0.576 - ETA: 0s - loss: 1.0189 - acc: 0.574 - ETA: 0s - loss: 1.0270 - acc: 0.570 - ETA: 0s - loss: 1.0235 - acc: 0.569 - ETA: 0s - loss: 1.0200 - acc: 0.572 - 2s 593us/step - loss: 1.0244 - acc: 0.5694 - val_loss: 1.8453 - val_acc: 0.2479\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.38710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2593b6efb00>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_learning_model_checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.transfer_learning_model.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "print(len(train_transfer_learning_model_features[:2624]))\n",
    "print(len(train_targets))\n",
    "\n",
    "print(len(valid_transfer_learning_model_features))\n",
    "print(len(valid_targets))\n",
    "\n",
    "train_transfer_learning_model_features = train_transfer_learning_model_features[:2624]\n",
    "valid_targets_2 = valid_targets[:835]\n",
    "      \n",
    "transfer_learning_model.fit(train_transfer_learning_model_features, train_targets, \n",
    "          validation_data=(valid_transfer_learning_model_features, valid_targets_2),\n",
    "          epochs=10, batch_size=20, callbacks=[transfer_learning_model_checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning_model.load_weights('saved_models/weights.best.transfer_learning_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.0000%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "transfer_learning_model_predictions = [np.argmax(transfer_learning_model.predict(np.expand_dims(feature, axis=0))) for feature in test_transfer_learning_model_features]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(transfer_learning_model_predictions)==np.argmax(test_targets, axis=1))/len(transfer_learning_model_predictions)\n",
    "\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
