{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resize data on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code snippet to resize data on disk and optimize limited computer resources such as RAM and disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #!/usr/bin/python\n",
    "### from PIL import Image\n",
    "### import os, sys\n",
    "### \n",
    "### path = \"OCT2017-RESIZED-V1/train/NORMAL/\"\n",
    "### dirs = os.listdir( path )\n",
    "### \n",
    "### def resize():\n",
    "###     for item in dirs:\n",
    "###         if os.path.isfile(path+item):\n",
    "###             im = Image.open(path+item)\n",
    "###             f, e = os.path.splitext(path+item)\n",
    "###             imResize = im.resize((224,224), Image.ANTIALIAS)\n",
    "###             imResize.save(f + '.jpeg', 'JPEG', quality=100)\n",
    "### \n",
    "### resize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import ImageFile    \n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = 'OCT2017-RESIZED-V1/train'\n",
    "valid_directory = 'OCT2017-RESIZED-V1/valid'\n",
    "test_directory = 'OCT2017-RESIZED-V1/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 total oct categories.\n",
      "There are 7020 total oct images.\n",
      "\n",
      "There are 5082 training oct images.\n",
      "There are 938 validation oct images.\n",
      "There are 1000 test oct images.\n"
     ]
    }
   ],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    oct_files = np.array(data['filenames'])\n",
    "    oct_targets = np_utils.to_categorical(np.array(data['target']), 4)\n",
    "    return oct_files, oct_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset(train_directory)\n",
    "valid_files, valid_targets = load_dataset(valid_directory)\n",
    "test_files, test_targets = load_dataset(test_directory)\n",
    "\n",
    "# load list of oct names\n",
    "oct_names = [item[20:-1] for item in sorted(glob(train_directory + \"/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total oct categories.' % len(oct_names))\n",
    "print('There are %s total oct images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training oct images.' % len(train_files))\n",
    "print('There are %d validation oct images.' % len(valid_files))\n",
    "print('There are %d test oct images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarities between images of different categories\n",
    "\n",
    "CNV | DME | DRUSEN | NORMAL\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/test/CNV/CNV-6256161-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/test/DME/DME-4940184-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/test/DRUSEN/DRUSEN-7373858-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/test/NORMAL/NORMAL-3103940-1.jpeg\" width=\"224\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinctive Images of CNV\n",
    "\n",
    "CNV | CNV | CNV | CNV\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/train/CNV/CNV-154835-109.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/CNV/CNV-154835-83.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/CNV/CNV-172472-41.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/CNV/CNV-172472-39.jpeg\" width=\"224\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinctive Images of DME\n",
    "\n",
    "DME | DME | DME | DME\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/train/DME/DME-306172-74.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DME/DME-323904-8.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DME/DME-462675-36.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DME/DME-633268-67.jpeg\" width=\"224\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinctive Images of DRUSEN\n",
    "\n",
    "DRUSEN | DRUSEN | DRUSEN | DRUSEN\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/train/DRUSEN/DRUSEN-457907-12.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DRUSEN/DRUSEN-2128644-16.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DRUSEN/DRUSEN-7106073-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DRUSEN/DRUSEN-8023853-38.jpeg\" width=\"224\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinctive Images of NORMAL\n",
    "\n",
    "NORMAL | NORMAL | NORMAL | NORMAL\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/train/NORMAL/NORMAL-216250-2.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/NORMAL/NORMAL-258763-36.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/NORMAL/NORMAL-286318-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/NORMAL/NORMAL-778975-37.jpeg\" width=\"224\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5082/5082 [00:08<00:00, 603.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [01:02<00:00, 15.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:25<00:00, 38.47it/s]\n"
     ]
    }
   ],
   "source": [
    "                        \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step0'></a>\n",
    "## Step N: Create benchmark model\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 75)      975       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 75)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 100)     30100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 125)       50125     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 125)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 125)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 98000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 392004    \n",
      "=================================================================\n",
      "Total params: 473,204\n",
      "Trainable params: 473,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    #Locally connected layer containing fewer weights\n",
    "    #Break the image up into smaller pieces\n",
    "    #Use 75 filters to identify the most general patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=75, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3)),\n",
    "    \n",
    "    #Reduce dimensionality of convolutional layer,\n",
    "    #Reduce by taking the maximum value in the filter\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    #Use 100 filters to identify the more specific patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=100, kernel_size=2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    #Use 125 filters to identify the more specific patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),\n",
    "    \n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    #MaxPooling2D(pool_size=2),\n",
    "    #Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    #MaxPooling2D(pool_size=2),\n",
    "    #Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    #MaxPooling2D(pool_size=2),    \n",
    "    #Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    #MaxPooling2D(pool_size=2),\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    # Add a softmax activation layer\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5082 samples, validate on 938 samples\n",
      "Epoch 1/7\n",
      " - 165s - loss: 1.1929 - acc: 0.4866 - val_loss: 1.4747 - val_acc: 0.3614\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.47466, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/7\n",
      " - 104s - loss: 0.7626 - acc: 0.7050 - val_loss: 1.1320 - val_acc: 0.5362\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.47466 to 1.13203, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/7\n",
      " - 103s - loss: 0.5408 - acc: 0.7934 - val_loss: 1.5254 - val_acc: 0.4414\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.13203\n",
      "Epoch 4/7\n",
      " - 104s - loss: 0.3929 - acc: 0.8597 - val_loss: 1.2960 - val_acc: 0.5608\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.13203\n",
      "Epoch 5/7\n",
      " - 104s - loss: 0.2858 - acc: 0.8981 - val_loss: 1.3064 - val_acc: 0.5650\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.13203\n",
      "Epoch 6/7\n",
      " - 104s - loss: 0.2097 - acc: 0.9256 - val_loss: 1.4265 - val_acc: 0.5458\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.13203\n",
      "Epoch 7/7\n",
      " - 104s - loss: 0.1549 - acc: 0.9453 - val_loss: 1.4501 - val_acc: 0.5565\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.13203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x233af50ceb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 7\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 58.8000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted diagnosis for each image in test set\n",
    "diagnosis_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(diagnosis_predictions)==np.argmax(test_targets, axis=1))/len(diagnosis_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
