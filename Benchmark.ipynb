{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resize data on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code snippet to resize data on disk and optimize limited computer resources such as RAM and disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #!/usr/bin/python\n",
    "### from PIL import Image\n",
    "### import os, sys\n",
    "### \n",
    "### path = \"OCT2017-RESIZED-V1/train/NORMAL/\"\n",
    "### dirs = os.listdir( path )\n",
    "### \n",
    "### def resize():\n",
    "###     for item in dirs:\n",
    "###         if os.path.isfile(path+item):\n",
    "###             im = Image.open(path+item)\n",
    "###             f, e = os.path.splitext(path+item)\n",
    "###             imResize = im.resize((224,224), Image.ANTIALIAS)\n",
    "###             imResize.save(f + '.jpeg', 'JPEG', quality=100)\n",
    "### \n",
    "### resize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import ImageFile    \n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = 'OCT2017-RESIZED-V1/train'\n",
    "valid_directory = 'OCT2017-RESIZED-V1/valid'\n",
    "test_directory = 'OCT2017-RESIZED-V1/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 total oct categories.\n",
      "There are 7020 total oct images.\n",
      "\n",
      "There are 5082 training oct images.\n",
      "There are 938 validation oct images.\n",
      "There are 1000 test oct images.\n"
     ]
    }
   ],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    oct_files = np.array(data['filenames'])\n",
    "    oct_targets = np_utils.to_categorical(np.array(data['target']), 4)\n",
    "    return oct_files, oct_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset(train_directory)\n",
    "valid_files, valid_targets = load_dataset(valid_directory)\n",
    "test_files, test_targets = load_dataset(test_directory)\n",
    "\n",
    "# load list of oct names\n",
    "oct_names = [item[20:-1] for item in sorted(glob(train_directory + \"/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total oct categories.' % len(oct_names))\n",
    "print('There are %s total oct images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training oct images.' % len(train_files))\n",
    "print('There are %d validation oct images.' % len(valid_files))\n",
    "print('There are %d test oct images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarities between images of different categories\n",
    "\n",
    "CNV | DME | DRUSEN | NORMAL\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/test/CNV/CNV-6256161-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/test/DME/DME-4940184-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/test/DRUSEN/DRUSEN-7373858-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/test/NORMAL/NORMAL-3103940-1.jpeg\" width=\"224\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinctive Images of CNV\n",
    "\n",
    "CNV | CNV | CNV | CNV\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/train/CNV/CNV-154835-109.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/CNV/CNV-154835-83.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/CNV/CNV-172472-41.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/CNV/CNV-172472-39.jpeg\" width=\"224\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinctive Images of DME\n",
    "\n",
    "DME | DME | DME | DME\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/train/DME/DME-306172-74.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DME/DME-323904-8.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DME/DME-462675-36.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DME/DME-633268-67.jpeg\" width=\"224\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinctive Images of DRUSEN\n",
    "\n",
    "DRUSEN | DRUSEN | DRUSEN | DRUSEN\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/train/DRUSEN/DRUSEN-457907-12.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DRUSEN/DRUSEN-2128644-16.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DRUSEN/DRUSEN-7106073-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/DRUSEN/DRUSEN-8023853-38.jpeg\" width=\"224\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinctive Images of NORMAL\n",
    "\n",
    "NORMAL | NORMAL | NORMAL | NORMAL\n",
    "- | -\n",
    "<img src=\"OCT2017-RESIZED-V1/train/NORMAL/NORMAL-216250-2.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/NORMAL/NORMAL-258763-36.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/NORMAL/NORMAL-286318-1.jpeg\" width=\"224\"> | <img src=\"OCT2017-RESIZED-V1/train/NORMAL/NORMAL-778975-37.jpeg\" width=\"224\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods to convert images into 4D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all data in tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5082/5082 [00:07<00:00, 647.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:16<00:00, 57.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:15<00:00, 65.88it/s]\n"
     ]
    }
   ],
   "source": [
    "                        \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step0'></a>\n",
    "## Benchmark models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Prediction Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://machinelearningmastery.com/implement-baseline-machine-learning-algorithms-scratch-python/\n",
    "\n",
    "The article refereced above mentions the Random Prediction Algorithm concept, but the implementation in it didn't match the OCT dataset, so I took the concept and built my own implementation to match the OCT dataset. See below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    " \n",
    "# Generate random predictions\n",
    "def random_algorithm(training_images):\n",
    "    predictions = list() # Create list to store all predictions\n",
    "    for training_image in training_images: # Loop through all training_images\n",
    "        image_random_prediction = [0, 0, 0, 0] # Create array of 4 indexes (1 index per class)\n",
    "        image_random_prediction[randrange(len(training_images[0]))] = 1 # Randomly select an index and set it to 1\n",
    "        predictions.append(image_random_prediction) # Append random selection to predictions list\n",
    "    return predictions # return all predictions\n",
    " \n",
    "seed(1)\n",
    "random_algorithm_predictions = random_algorithm(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 24.9115%\n"
     ]
    }
   ],
   "source": [
    "# report test accuracy\n",
    "\n",
    "\n",
    "random_algorithm_accuracy = 100*np.sum(np.argmax(train_targets, axis=1)==np.argmax(random_algorithm_predictions, axis=1))/len(train_targets)\n",
    "print('Test accuracy: %.4f%%' % random_algorithm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Rule Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://machinelearningmastery.com/implement-baseline-machine-learning-algorithms-scratch-python/\n",
    "\n",
    "The article refereced above mentions the Zero Rule Algorithm concept, but the implementation in it didn't match the OCT dataset, so I took the concept and built my own implementation to match the OCT dataset. See below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def zero_rule_algorithm_classification(training_images):\n",
    "    predictions = list() # Create list to store all predictions\n",
    "    \n",
    "    # Count files in directories\n",
    "    cnv_total_files = len(os.listdir(train_directory + '/CNV/'))\n",
    "    dme_total_files = len(os.listdir(train_directory + '/DME/'))\n",
    "    drusen_total_files = len(os.listdir(train_directory + '/DRUSEN/'))\n",
    "    normal_total_files = len(os.listdir(train_directory + '/NORMAL/'))\n",
    "    \n",
    "    # Create array with the total of each category\n",
    "    total_classification_files = [cnv_total_files, dme_total_files, drusen_total_files, normal_total_files]\n",
    "    index_with_most_images = np.argmax(total_classification_files, axis=0)\n",
    "    \n",
    "    for training_image in training_images: # Loop through all training_images\n",
    "        image_prediction = [0, 0, 0, 0] # Create array of 4 indexes (1 index per class)\n",
    "        image_prediction[index_with_most_images] = 1 # Select the index with the most images and set it to 1\n",
    "        predictions.append(image_prediction) # Append prediction to list\n",
    "    return predictions # return all predictions\n",
    " \n",
    "zero_rule_algorithm_classification_predictions = zero_rule_algorithm_classification(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 32.1921%\n"
     ]
    }
   ],
   "source": [
    "# report test accuracy\n",
    "\n",
    "\n",
    "zero_rule_algorithm_classification_accuracy = 100*np.sum(np.argmax(train_targets, axis=1)==np.argmax(zero_rule_algorithm_classification_predictions, axis=1))/len(train_targets)\n",
    "print('Test accuracy: %.4f%%' % zero_rule_algorithm_classification_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a CNN from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many convolutional layers and why?\n",
    "- I added 3 convolutional layers to have the ability to extract the most important patterns in the images. \n",
    "These patterns are later used to predict the categories or objects in new images. \n",
    "The 2 deeper convolutional layers allow the architecture to find patterns within the patterns.\n",
    "\n",
    "How you decided the kernel size and strides?\n",
    "- I decided to keep the default stride of 1. \n",
    "Because, I believe, this will allow to keep as much data of the original image as possible. \n",
    "\n",
    "- I deviced to use a kernel size of 2x2 also to try to keep as much information as possible, \n",
    "to reduce training time and to preserve imporant image information that will allow the \n",
    "architecture to achive an acceptable level of accuracy. \n",
    "\n",
    "Why the Dropout layers?\n",
    "- The dropout layer at the later end of the architecture is to minimize overfitting. I chose to have a probability of 0.3 that corresponds to the probability that any node in the network is removed during the training. \n",
    "\n",
    "Why the Max Pool layers?\n",
    "- The pooling layers after each convolutional layer have the purpose of reducing the dimensionality of the feature set, \n",
    "which can lead to overfitting. There are 2 types of pooling layers: \n",
    "    - MaxPoolingLayer and GlobalAveragePoolingLayer. \n",
    "- I choose the MaxPooling layer to reduce the computation time and just take the largest value in the matrix. \n",
    "\n",
    "What is the purpose of flatten layer?\n",
    "- The purpose of the flatten layer is to take the image matrix and convert it into a vector. It converts the image into a vector so that it can be used in the final Dense layer.\n",
    "\n",
    "What are the dense layers doing, and how you decided the number of dense layers?\n",
    "- The last dense layer has 4 nodes to get the final probabilities for each possible OCT diagnosis in our dataset. \n",
    "- I used a softmax activation function, because this is a multi-class classification problem. It ensures that the network outputs an estimate for the probability that each potential breed is depicted in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 75)      975       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 75)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 100)     30100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 125)       50125     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 125)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 125)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 98000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 392004    \n",
      "=================================================================\n",
      "Total params: 473,204\n",
      "Trainable params: 473,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    #Locally connected layer containing fewer weights\n",
    "    #Break the image up into smaller pieces\n",
    "    #Use 75 filters to identify the most general patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=75, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3)),\n",
    "    \n",
    "    #Reduce dimensionality of convolutional layer,\n",
    "    #Reduce by taking the maximum value in the filter\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    #Use 100 filters to identify the more specific patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=100, kernel_size=2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    #Use 125 filters to identify the more specific patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),\n",
    "    \n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    \n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    # Add a softmax activation layer\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5082 samples, validate on 938 samples\n",
      "Epoch 1/7\n",
      " - 131s - loss: 1.1948 - acc: 0.4852 - val_loss: 1.3844 - val_acc: 0.3081\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38436, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/7\n",
      " - 103s - loss: 0.7438 - acc: 0.7137 - val_loss: 1.3415 - val_acc: 0.5053\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.38436 to 1.34149, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/7\n",
      " - 103s - loss: 0.5325 - acc: 0.8119 - val_loss: 1.2200 - val_acc: 0.5192\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.34149 to 1.21996, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/7\n",
      " - 103s - loss: 0.3836 - acc: 0.8672 - val_loss: 1.0626 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.21996 to 1.06258, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/7\n",
      " - 103s - loss: 0.2783 - acc: 0.9038 - val_loss: 1.5075 - val_acc: 0.5480\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.06258\n",
      "Epoch 6/7\n",
      " - 103s - loss: 0.2036 - acc: 0.9284 - val_loss: 1.4169 - val_acc: 0.6151\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.06258\n",
      "Epoch 7/7\n",
      " - 103s - loss: 0.1453 - acc: 0.9522 - val_loss: 1.4018 - val_acc: 0.6066\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.06258\n",
      "Ellapsed: 0:12:33.111347\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=2)\n",
    "\n",
    "\n",
    "print('Ellapsed: ' + str(datetime.datetime.now() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model's best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 70.6000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted diagnosis for each image in test set\n",
    "diagnosis_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(diagnosis_predictions)==np.argmax(test_targets, axis=1))/len(diagnosis_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
