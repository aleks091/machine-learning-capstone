{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3 transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 total oct categories.\n",
      "There are 7020 total oct images.\n",
      "\n",
      "There are 5082 training oct images.\n",
      "There are 938 validation oct images.\n",
      "There are 1000 test oct images.\n",
      "Ellapsed: 0:01:26.594943\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    oct_files = np.array(data['filenames'])\n",
    "    oct_targets = np_utils.to_categorical(np.array(data['target']), 4)\n",
    "    return oct_files, oct_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('OCT2017-RESIZED-V1/train')\n",
    "valid_files, valid_targets = load_dataset('OCT2017-RESIZED-V1/valid')\n",
    "test_files, test_targets = load_dataset('OCT2017-RESIZED-V1/test')\n",
    "\n",
    "# load list of oct names\n",
    "oct_names = [item[20:-1] for item in sorted(glob(\"OCT2017-RESIZED-V1/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total oct categories.' % len(oct_names))\n",
    "print('There are %s total oct images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training oct images.' % len(train_files))\n",
    "print('There are %d validation oct images.' % len(valid_files))\n",
    "print('There are %d test oct images.'% len(test_files))\n",
    "\n",
    "print('Ellapsed: ' + str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5082/5082 [00:08<00:00, 592.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:20<00:00, 46.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:24<00:00, 40.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ellapsed: 0:01:06.932360\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "print('Ellapsed: ' + str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "### Utilize InceptionV3 pretrained model to predict OCT diagnosis\n",
    "\n",
    "\n",
    "- [InceptionV3](https://keras.io/applications/#inceptionv3) pretrained on imagenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.InceptionV3.hdf5', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate InceptionV3 model pretrained on imagenet\n",
    "\n",
    "##### InceptionV3 parameters \n",
    "\n",
    "- include_top set to false to fine-tune the last layers to make predictions on the 4 classification categories\n",
    "\n",
    "##### Create architecture and train model\n",
    "\n",
    "- Freeze the first 200 layers to use pretrain weights \n",
    "- Enable training on the rest 110 layers. \n",
    "- Compile the model using the following parameters:\n",
    "    - optimizer = rmsprop\n",
    "    - loss = categorical_crossentropy\n",
    "    \n",
    "- Train the model \n",
    "\n",
    "- Repeat process\n",
    "    - Freeze the first 200 layers to use pretrain weights \n",
    "    - Enable training on the rest 110 layers. \n",
    "    - Compile the model using the following parameters:\n",
    "        - optimizer = Stochastic Gradient Descent with learning rate 0.0001 and momentum=0.9\n",
    "        - loss= categorical_crossentropy\n",
    "- Train model\n",
    "\n",
    "- epochs was set to 5\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5082 samples, validate on 938 samples\n",
      "Epoch 1/5\n",
      " - 157s - loss: 1.1069 - val_loss: 4.8554\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.85537, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "Epoch 2/5\n",
      " - 129s - loss: 0.2916 - val_loss: 4.7865\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.85537 to 4.78649, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "Epoch 3/5\n",
      " - 129s - loss: 0.1940 - val_loss: 3.8102\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.78649 to 3.81024, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "Epoch 4/5\n",
      " - 129s - loss: 0.1446 - val_loss: 6.2039\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.81024\n",
      "Epoch 5/5\n",
      " - 129s - loss: 0.3720 - val_loss: 3.5182\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.81024 to 3.51815, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "0 input_1 False\n",
      "1 conv2d_1 False\n",
      "2 batch_normalization_1 False\n",
      "3 activation_1 False\n",
      "4 conv2d_2 False\n",
      "5 batch_normalization_2 False\n",
      "6 activation_2 False\n",
      "7 conv2d_3 False\n",
      "8 batch_normalization_3 False\n",
      "9 activation_3 False\n",
      "10 max_pooling2d_1 False\n",
      "11 conv2d_4 False\n",
      "12 batch_normalization_4 False\n",
      "13 activation_4 False\n",
      "14 conv2d_5 False\n",
      "15 batch_normalization_5 False\n",
      "16 activation_5 False\n",
      "17 max_pooling2d_2 False\n",
      "18 conv2d_9 False\n",
      "19 batch_normalization_9 False\n",
      "20 activation_9 False\n",
      "21 conv2d_7 False\n",
      "22 conv2d_10 False\n",
      "23 batch_normalization_7 False\n",
      "24 batch_normalization_10 False\n",
      "25 activation_7 False\n",
      "26 activation_10 False\n",
      "27 average_pooling2d_1 False\n",
      "28 conv2d_6 False\n",
      "29 conv2d_8 False\n",
      "30 conv2d_11 False\n",
      "31 conv2d_12 False\n",
      "32 batch_normalization_6 False\n",
      "33 batch_normalization_8 False\n",
      "34 batch_normalization_11 False\n",
      "35 batch_normalization_12 False\n",
      "36 activation_6 False\n",
      "37 activation_8 False\n",
      "38 activation_11 False\n",
      "39 activation_12 False\n",
      "40 mixed0 False\n",
      "41 conv2d_16 False\n",
      "42 batch_normalization_16 False\n",
      "43 activation_16 False\n",
      "44 conv2d_14 False\n",
      "45 conv2d_17 False\n",
      "46 batch_normalization_14 False\n",
      "47 batch_normalization_17 False\n",
      "48 activation_14 False\n",
      "49 activation_17 False\n",
      "50 average_pooling2d_2 False\n",
      "51 conv2d_13 False\n",
      "52 conv2d_15 False\n",
      "53 conv2d_18 False\n",
      "54 conv2d_19 False\n",
      "55 batch_normalization_13 False\n",
      "56 batch_normalization_15 False\n",
      "57 batch_normalization_18 False\n",
      "58 batch_normalization_19 False\n",
      "59 activation_13 False\n",
      "60 activation_15 False\n",
      "61 activation_18 False\n",
      "62 activation_19 False\n",
      "63 mixed1 False\n",
      "64 conv2d_23 False\n",
      "65 batch_normalization_23 False\n",
      "66 activation_23 False\n",
      "67 conv2d_21 False\n",
      "68 conv2d_24 False\n",
      "69 batch_normalization_21 False\n",
      "70 batch_normalization_24 False\n",
      "71 activation_21 False\n",
      "72 activation_24 False\n",
      "73 average_pooling2d_3 False\n",
      "74 conv2d_20 False\n",
      "75 conv2d_22 False\n",
      "76 conv2d_25 False\n",
      "77 conv2d_26 False\n",
      "78 batch_normalization_20 False\n",
      "79 batch_normalization_22 False\n",
      "80 batch_normalization_25 False\n",
      "81 batch_normalization_26 False\n",
      "82 activation_20 False\n",
      "83 activation_22 False\n",
      "84 activation_25 False\n",
      "85 activation_26 False\n",
      "86 mixed2 False\n",
      "87 conv2d_28 False\n",
      "88 batch_normalization_28 False\n",
      "89 activation_28 False\n",
      "90 conv2d_29 False\n",
      "91 batch_normalization_29 False\n",
      "92 activation_29 False\n",
      "93 conv2d_27 False\n",
      "94 conv2d_30 False\n",
      "95 batch_normalization_27 False\n",
      "96 batch_normalization_30 False\n",
      "97 activation_27 False\n",
      "98 activation_30 False\n",
      "99 max_pooling2d_3 False\n",
      "100 mixed3 False\n",
      "101 conv2d_35 False\n",
      "102 batch_normalization_35 False\n",
      "103 activation_35 False\n",
      "104 conv2d_36 False\n",
      "105 batch_normalization_36 False\n",
      "106 activation_36 False\n",
      "107 conv2d_32 False\n",
      "108 conv2d_37 False\n",
      "109 batch_normalization_32 False\n",
      "110 batch_normalization_37 False\n",
      "111 activation_32 False\n",
      "112 activation_37 False\n",
      "113 conv2d_33 False\n",
      "114 conv2d_38 False\n",
      "115 batch_normalization_33 False\n",
      "116 batch_normalization_38 False\n",
      "117 activation_33 False\n",
      "118 activation_38 False\n",
      "119 average_pooling2d_4 False\n",
      "120 conv2d_31 False\n",
      "121 conv2d_34 False\n",
      "122 conv2d_39 False\n",
      "123 conv2d_40 False\n",
      "124 batch_normalization_31 False\n",
      "125 batch_normalization_34 False\n",
      "126 batch_normalization_39 False\n",
      "127 batch_normalization_40 False\n",
      "128 activation_31 False\n",
      "129 activation_34 False\n",
      "130 activation_39 False\n",
      "131 activation_40 False\n",
      "132 mixed4 False\n",
      "133 conv2d_45 False\n",
      "134 batch_normalization_45 False\n",
      "135 activation_45 False\n",
      "136 conv2d_46 False\n",
      "137 batch_normalization_46 False\n",
      "138 activation_46 False\n",
      "139 conv2d_42 False\n",
      "140 conv2d_47 False\n",
      "141 batch_normalization_42 False\n",
      "142 batch_normalization_47 False\n",
      "143 activation_42 False\n",
      "144 activation_47 False\n",
      "145 conv2d_43 False\n",
      "146 conv2d_48 False\n",
      "147 batch_normalization_43 False\n",
      "148 batch_normalization_48 False\n",
      "149 activation_43 False\n",
      "150 activation_48 False\n",
      "151 average_pooling2d_5 False\n",
      "152 conv2d_41 False\n",
      "153 conv2d_44 False\n",
      "154 conv2d_49 False\n",
      "155 conv2d_50 False\n",
      "156 batch_normalization_41 False\n",
      "157 batch_normalization_44 False\n",
      "158 batch_normalization_49 False\n",
      "159 batch_normalization_50 False\n",
      "160 activation_41 False\n",
      "161 activation_44 False\n",
      "162 activation_49 False\n",
      "163 activation_50 False\n",
      "164 mixed5 False\n",
      "165 conv2d_55 False\n",
      "166 batch_normalization_55 False\n",
      "167 activation_55 False\n",
      "168 conv2d_56 False\n",
      "169 batch_normalization_56 False\n",
      "170 activation_56 False\n",
      "171 conv2d_52 False\n",
      "172 conv2d_57 False\n",
      "173 batch_normalization_52 False\n",
      "174 batch_normalization_57 False\n",
      "175 activation_52 False\n",
      "176 activation_57 False\n",
      "177 conv2d_53 False\n",
      "178 conv2d_58 False\n",
      "179 batch_normalization_53 False\n",
      "180 batch_normalization_58 False\n",
      "181 activation_53 False\n",
      "182 activation_58 False\n",
      "183 average_pooling2d_6 False\n",
      "184 conv2d_51 False\n",
      "185 conv2d_54 False\n",
      "186 conv2d_59 False\n",
      "187 conv2d_60 False\n",
      "188 batch_normalization_51 False\n",
      "189 batch_normalization_54 False\n",
      "190 batch_normalization_59 False\n",
      "191 batch_normalization_60 False\n",
      "192 activation_51 False\n",
      "193 activation_54 False\n",
      "194 activation_59 False\n",
      "195 activation_60 False\n",
      "196 mixed6 False\n",
      "197 conv2d_65 False\n",
      "198 batch_normalization_65 False\n",
      "199 activation_65 False\n",
      "200 conv2d_66 True\n",
      "201 batch_normalization_66 True\n",
      "202 activation_66 True\n",
      "203 conv2d_62 True\n",
      "204 conv2d_67 True\n",
      "205 batch_normalization_62 True\n",
      "206 batch_normalization_67 True\n",
      "207 activation_62 True\n",
      "208 activation_67 True\n",
      "209 conv2d_63 True\n",
      "210 conv2d_68 True\n",
      "211 batch_normalization_63 True\n",
      "212 batch_normalization_68 True\n",
      "213 activation_63 True\n",
      "214 activation_68 True\n",
      "215 average_pooling2d_7 True\n",
      "216 conv2d_61 True\n",
      "217 conv2d_64 True\n",
      "218 conv2d_69 True\n",
      "219 conv2d_70 True\n",
      "220 batch_normalization_61 True\n",
      "221 batch_normalization_64 True\n",
      "222 batch_normalization_69 True\n",
      "223 batch_normalization_70 True\n",
      "224 activation_61 True\n",
      "225 activation_64 True\n",
      "226 activation_69 True\n",
      "227 activation_70 True\n",
      "228 mixed7 True\n",
      "229 conv2d_73 True\n",
      "230 batch_normalization_73 True\n",
      "231 activation_73 True\n",
      "232 conv2d_74 True\n",
      "233 batch_normalization_74 True\n",
      "234 activation_74 True\n",
      "235 conv2d_71 True\n",
      "236 conv2d_75 True\n",
      "237 batch_normalization_71 True\n",
      "238 batch_normalization_75 True\n",
      "239 activation_71 True\n",
      "240 activation_75 True\n",
      "241 conv2d_72 True\n",
      "242 conv2d_76 True\n",
      "243 batch_normalization_72 True\n",
      "244 batch_normalization_76 True\n",
      "245 activation_72 True\n",
      "246 activation_76 True\n",
      "247 max_pooling2d_4 True\n",
      "248 mixed8 True\n",
      "249 conv2d_81 True\n",
      "250 batch_normalization_81 True\n",
      "251 activation_81 True\n",
      "252 conv2d_78 True\n",
      "253 conv2d_82 True\n",
      "254 batch_normalization_78 True\n",
      "255 batch_normalization_82 True\n",
      "256 activation_78 True\n",
      "257 activation_82 True\n",
      "258 conv2d_79 True\n",
      "259 conv2d_80 True\n",
      "260 conv2d_83 True\n",
      "261 conv2d_84 True\n",
      "262 average_pooling2d_8 True\n",
      "263 conv2d_77 True\n",
      "264 batch_normalization_79 True\n",
      "265 batch_normalization_80 True\n",
      "266 batch_normalization_83 True\n",
      "267 batch_normalization_84 True\n",
      "268 conv2d_85 True\n",
      "269 batch_normalization_77 True\n",
      "270 activation_79 True\n",
      "271 activation_80 True\n",
      "272 activation_83 True\n",
      "273 activation_84 True\n",
      "274 batch_normalization_85 True\n",
      "275 activation_77 True\n",
      "276 mixed9_0 True\n",
      "277 concatenate_1 True\n",
      "278 activation_85 True\n",
      "279 mixed9 True\n",
      "280 conv2d_90 True\n",
      "281 batch_normalization_90 True\n",
      "282 activation_90 True\n",
      "283 conv2d_87 True\n",
      "284 conv2d_91 True\n",
      "285 batch_normalization_87 True\n",
      "286 batch_normalization_91 True\n",
      "287 activation_87 True\n",
      "288 activation_91 True\n",
      "289 conv2d_88 True\n",
      "290 conv2d_89 True\n",
      "291 conv2d_92 True\n",
      "292 conv2d_93 True\n",
      "293 average_pooling2d_9 True\n",
      "294 conv2d_86 True\n",
      "295 batch_normalization_88 True\n",
      "296 batch_normalization_89 True\n",
      "297 batch_normalization_92 True\n",
      "298 batch_normalization_93 True\n",
      "299 conv2d_94 True\n",
      "300 batch_normalization_86 True\n",
      "301 activation_88 True\n",
      "302 activation_89 True\n",
      "303 activation_92 True\n",
      "304 activation_93 True\n",
      "305 batch_normalization_94 True\n",
      "306 activation_86 True\n",
      "307 mixed9_1 True\n",
      "308 concatenate_2 True\n",
      "309 activation_94 True\n",
      "310 mixed10 True\n",
      "Train on 5082 samples, validate on 938 samples\n",
      "Epoch 1/5\n",
      " - 127s - loss: 0.0937 - val_loss: 2.3105\n",
      "\n",
      "Epoch 00001: val_loss improved from 3.51815 to 2.31046, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "Epoch 2/5\n",
      " - 122s - loss: 0.0970 - val_loss: 2.3540\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.31046\n",
      "Epoch 3/5\n",
      " - 122s - loss: 0.0895 - val_loss: 2.3221\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.31046\n",
      "Epoch 4/5\n",
      " - 122s - loss: 0.0598 - val_loss: 2.3570\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.31046\n",
      "Epoch 5/5\n",
      " - 122s - loss: 0.0610 - val_loss: 2.3419\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.31046\n",
      "Ellapsed: 0:22:46.193815\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "#InceptionV\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Create the layer to make predictions on the 4 different classes using a softmax activation function.\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers[:200]:\n",
    "   layer.trainable = False\n",
    "for layer in base_model.layers[200:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=2)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name, layer.trainable)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 200 layers and unfreeze the rest:\n",
    "for layer in model.layers[:200]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[200:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=2)\n",
    "\n",
    "\n",
    "print('Ellapsed: ' + str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.InceptionV3.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.5000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "diagnosis_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(diagnosis_predictions)==np.argmax(test_targets, axis=1))/len(diagnosis_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
