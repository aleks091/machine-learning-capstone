{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Capstone Project: Classification of retinal optical coherence tomography diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Step 0: Verify TensorFlow is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 1: Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 total oct categories.\n",
      "There are 2681 total oct images.\n",
      "\n",
      "There are 1184 training oct images.\n",
      "There are 938 validation oct images.\n",
      "There are 559 test oct images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    oct_files = np.array(data['filenames'])\n",
    "    oct_targets = np_utils.to_categorical(np.array(data['target']), 4)\n",
    "    return oct_files, oct_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('OCT2017-R2/train')\n",
    "valid_files, valid_targets = load_dataset('OCT2017-R2/valid')\n",
    "test_files, test_targets = load_dataset('OCT2017-R2/test')\n",
    "\n",
    "# load list of oct names\n",
    "oct_names = [item[20:-1] for item in sorted(glob(\"OCT2017-R2/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total oct categories.' % len(oct_names))\n",
    "print('There are %s total oct images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training oct images.' % len(train_files))\n",
    "print('There are %d validation oct images.' % len(valid_files))\n",
    "print('There are %d test oct images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1184/1184 [00:05<00:00, 230.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 938/938 [00:03<00:00, 256.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 559/559 [00:01<00:00, 306.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step0'></a>\n",
    "## Step N: Create benchmark model\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 75)      975       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 75)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 100)     30100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 125)       50125     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 125)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 125)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 98000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 392004    \n",
      "=================================================================\n",
      "Total params: 473,204\n",
      "Trainable params: 473,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model = Sequential([\n",
    "    \n",
    "    #Locally connected layer containing fewer weights\n",
    "    #Break the image up into smaller pieces\n",
    "    #Use 75 filters to identify the most general patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=75, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3)),\n",
    "    \n",
    "    #Reduce dimensionality of convolutional layer,\n",
    "    #Reduce by taking the maximum value in the filter\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    #Use 100 filters to identify the more specific patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=100, kernel_size=2, padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    #Use 125 filters to identify the more specific patterns\n",
    "    #Use standard kerner_size of 2\n",
    "    Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),\n",
    "    \n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    #MaxPooling2D(pool_size=2),\n",
    "    #Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    #MaxPooling2D(pool_size=2),\n",
    "    #Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    #MaxPooling2D(pool_size=2),    \n",
    "    #Conv2D(filters=125, kernel_size=2, padding='same', activation='relu'),    \n",
    "    #MaxPooling2D(pool_size=2),\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    # Add a softmax activation layer\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2624 samples, validate on 938 samples\n",
      "Epoch 1/4\n",
      "2624/2624 [==============================] - ETA: 27:45 - loss: 1.3919 - acc: 0.25 - ETA: 14:13 - loss: 3.0511 - acc: 0.22 - ETA: 9:41 - loss: 2.6020 - acc: 0.2167 - ETA: 7:25 - loss: 2.2955 - acc: 0.250 - ETA: 6:03 - loss: 2.1275 - acc: 0.220 - ETA: 5:08 - loss: 2.0084 - acc: 0.216 - ETA: 4:29 - loss: 1.9244 - acc: 0.214 - ETA: 3:59 - loss: 1.8474 - acc: 0.237 - ETA: 3:36 - loss: 1.7986 - acc: 0.250 - ETA: 3:17 - loss: 1.7510 - acc: 0.260 - ETA: 3:02 - loss: 1.7214 - acc: 0.263 - ETA: 2:50 - loss: 1.6976 - acc: 0.250 - ETA: 2:39 - loss: 1.6688 - acc: 0.253 - ETA: 2:29 - loss: 1.6478 - acc: 0.260 - ETA: 2:21 - loss: 1.6319 - acc: 0.253 - ETA: 2:14 - loss: 1.6125 - acc: 0.253 - ETA: 2:07 - loss: 1.5961 - acc: 0.261 - ETA: 2:02 - loss: 1.5933 - acc: 0.255 - ETA: 1:57 - loss: 1.5828 - acc: 0.247 - ETA: 1:52 - loss: 1.5692 - acc: 0.255 - ETA: 1:48 - loss: 1.5614 - acc: 0.252 - ETA: 1:44 - loss: 1.5507 - acc: 0.252 - ETA: 1:40 - loss: 1.5390 - acc: 0.258 - ETA: 1:37 - loss: 1.5303 - acc: 0.256 - ETA: 1:34 - loss: 1.5199 - acc: 0.262 - ETA: 1:31 - loss: 1.5112 - acc: 0.267 - ETA: 1:28 - loss: 1.5084 - acc: 0.268 - ETA: 1:25 - loss: 1.5005 - acc: 0.278 - ETA: 1:23 - loss: 1.4921 - acc: 0.284 - ETA: 1:21 - loss: 1.4902 - acc: 0.281 - ETA: 1:19 - loss: 1.4899 - acc: 0.280 - ETA: 1:17 - loss: 1.4864 - acc: 0.282 - ETA: 1:15 - loss: 1.4814 - acc: 0.284 - ETA: 1:13 - loss: 1.4789 - acc: 0.283 - ETA: 1:11 - loss: 1.4742 - acc: 0.288 - ETA: 1:09 - loss: 1.4687 - acc: 0.291 - ETA: 1:08 - loss: 1.4670 - acc: 0.290 - ETA: 1:06 - loss: 1.4637 - acc: 0.293 - ETA: 1:05 - loss: 1.4594 - acc: 0.297 - ETA: 1:03 - loss: 1.4559 - acc: 0.300 - ETA: 1:02 - loss: 1.4529 - acc: 0.301 - ETA: 1:01 - loss: 1.4477 - acc: 0.307 - ETA: 59s - loss: 1.4479 - acc: 0.303 - ETA: 58s - loss: 1.4448 - acc: 0.30 - ETA: 57s - loss: 1.4413 - acc: 0.30 - ETA: 56s - loss: 1.4378 - acc: 0.30 - ETA: 54s - loss: 1.4396 - acc: 0.30 - ETA: 53s - loss: 1.4382 - acc: 0.30 - ETA: 52s - loss: 1.4354 - acc: 0.30 - ETA: 51s - loss: 1.4338 - acc: 0.30 - ETA: 50s - loss: 1.4316 - acc: 0.30 - ETA: 49s - loss: 1.4289 - acc: 0.30 - ETA: 48s - loss: 1.4241 - acc: 0.31 - ETA: 47s - loss: 1.4218 - acc: 0.31 - ETA: 46s - loss: 1.4246 - acc: 0.31 - ETA: 45s - loss: 1.4237 - acc: 0.31 - ETA: 44s - loss: 1.4223 - acc: 0.31 - ETA: 44s - loss: 1.4193 - acc: 0.31 - ETA: 43s - loss: 1.4141 - acc: 0.31 - ETA: 42s - loss: 1.4135 - acc: 0.31 - ETA: 41s - loss: 1.4122 - acc: 0.31 - ETA: 40s - loss: 1.4088 - acc: 0.32 - ETA: 39s - loss: 1.4055 - acc: 0.32 - ETA: 39s - loss: 1.4050 - acc: 0.32 - ETA: 38s - loss: 1.4042 - acc: 0.32 - ETA: 37s - loss: 1.4013 - acc: 0.33 - ETA: 36s - loss: 1.3976 - acc: 0.33 - ETA: 36s - loss: 1.3944 - acc: 0.33 - ETA: 35s - loss: 1.3945 - acc: 0.33 - ETA: 34s - loss: 1.3947 - acc: 0.33 - ETA: 33s - loss: 1.3960 - acc: 0.33 - ETA: 33s - loss: 1.3940 - acc: 0.33 - ETA: 32s - loss: 1.3917 - acc: 0.33 - ETA: 31s - loss: 1.3897 - acc: 0.34 - ETA: 31s - loss: 1.3868 - acc: 0.34 - ETA: 30s - loss: 1.3848 - acc: 0.34 - ETA: 29s - loss: 1.3843 - acc: 0.34 - ETA: 29s - loss: 1.3812 - acc: 0.34 - ETA: 28s - loss: 1.3782 - acc: 0.34 - ETA: 27s - loss: 1.3784 - acc: 0.34 - ETA: 27s - loss: 1.3760 - acc: 0.34 - ETA: 26s - loss: 1.3743 - acc: 0.34 - ETA: 25s - loss: 1.3735 - acc: 0.34 - ETA: 25s - loss: 1.3720 - acc: 0.35 - ETA: 24s - loss: 1.3700 - acc: 0.35 - ETA: 24s - loss: 1.3693 - acc: 0.35 - ETA: 23s - loss: 1.3683 - acc: 0.35 - ETA: 22s - loss: 1.3652 - acc: 0.35 - ETA: 22s - loss: 1.3630 - acc: 0.35 - ETA: 21s - loss: 1.3625 - acc: 0.35 - ETA: 21s - loss: 1.3617 - acc: 0.35 - ETA: 20s - loss: 1.3622 - acc: 0.35 - ETA: 19s - loss: 1.3611 - acc: 0.35 - ETA: 19s - loss: 1.3606 - acc: 0.36 - ETA: 18s - loss: 1.3594 - acc: 0.36 - ETA: 18s - loss: 1.3577 - acc: 0.36 - ETA: 17s - loss: 1.3556 - acc: 0.36 - ETA: 17s - loss: 1.3549 - acc: 0.36 - ETA: 16s - loss: 1.3533 - acc: 0.36 - ETA: 15s - loss: 1.3527 - acc: 0.36 - ETA: 15s - loss: 1.3495 - acc: 0.36 - ETA: 14s - loss: 1.3483 - acc: 0.36 - ETA: 14s - loss: 1.3473 - acc: 0.36 - ETA: 13s - loss: 1.3449 - acc: 0.37 - ETA: 13s - loss: 1.3439 - acc: 0.37 - ETA: 12s - loss: 1.3438 - acc: 0.37 - ETA: 12s - loss: 1.3416 - acc: 0.37 - ETA: 11s - loss: 1.3400 - acc: 0.37 - ETA: 11s - loss: 1.3374 - acc: 0.37 - ETA: 10s - loss: 1.3371 - acc: 0.37 - ETA: 10s - loss: 1.3358 - acc: 0.37 - ETA: 9s - loss: 1.3335 - acc: 0.3781 - ETA: 9s - loss: 1.3295 - acc: 0.380 - ETA: 8s - loss: 1.3283 - acc: 0.379 - ETA: 8s - loss: 1.3246 - acc: 0.382 - ETA: 7s - loss: 1.3242 - acc: 0.381 - ETA: 6s - loss: 1.3217 - acc: 0.383 - ETA: 6s - loss: 1.3190 - acc: 0.384 - ETA: 5s - loss: 1.3186 - acc: 0.384 - ETA: 5s - loss: 1.3208 - acc: 0.383 - ETA: 4s - loss: 1.3195 - acc: 0.385 - ETA: 4s - loss: 1.3194 - acc: 0.386 - ETA: 3s - loss: 1.3172 - acc: 0.388 - ETA: 3s - loss: 1.3162 - acc: 0.388 - ETA: 3s - loss: 1.3135 - acc: 0.390 - ETA: 2s - loss: 1.3115 - acc: 0.390 - ETA: 2s - loss: 1.3101 - acc: 0.390 - ETA: 1s - loss: 1.3083 - acc: 0.391 - ETA: 1s - loss: 1.3063 - acc: 0.393 - ETA: 0s - loss: 1.3049 - acc: 0.394 - ETA: 0s - loss: 1.3043 - acc: 0.395 - 71s 27ms/step - loss: 1.3041 - acc: 0.3948 - val_loss: 2.2998 - val_acc: 0.2996\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.29984, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/4\n",
      "2624/2624 [==============================] - ETA: 51s - loss: 1.8080 - acc: 0.40 - ETA: 50s - loss: 1.5198 - acc: 0.50 - ETA: 49s - loss: 1.4475 - acc: 0.45 - ETA: 49s - loss: 1.3457 - acc: 0.46 - ETA: 48s - loss: 1.3131 - acc: 0.45 - ETA: 48s - loss: 1.2715 - acc: 0.46 - ETA: 47s - loss: 1.2149 - acc: 0.50 - ETA: 47s - loss: 1.1676 - acc: 0.52 - ETA: 47s - loss: 1.1165 - acc: 0.55 - ETA: 46s - loss: 1.1253 - acc: 0.54 - ETA: 46s - loss: 1.1199 - acc: 0.53 - ETA: 45s - loss: 1.1093 - acc: 0.53 - ETA: 45s - loss: 1.1099 - acc: 0.52 - ETA: 45s - loss: 1.1171 - acc: 0.52 - ETA: 44s - loss: 1.1109 - acc: 0.52 - ETA: 44s - loss: 1.1017 - acc: 0.52 - ETA: 43s - loss: 1.0991 - acc: 0.52 - ETA: 43s - loss: 1.0871 - acc: 0.53 - ETA: 43s - loss: 1.0818 - acc: 0.52 - ETA: 42s - loss: 1.0937 - acc: 0.52 - ETA: 42s - loss: 1.0905 - acc: 0.52 - ETA: 42s - loss: 1.1002 - acc: 0.52 - ETA: 41s - loss: 1.0990 - acc: 0.52 - ETA: 41s - loss: 1.0929 - acc: 0.52 - ETA: 40s - loss: 1.0963 - acc: 0.52 - ETA: 40s - loss: 1.0896 - acc: 0.52 - ETA: 40s - loss: 1.0850 - acc: 0.52 - ETA: 39s - loss: 1.0814 - acc: 0.52 - ETA: 39s - loss: 1.0774 - acc: 0.52 - ETA: 39s - loss: 1.0728 - acc: 0.53 - ETA: 38s - loss: 1.0636 - acc: 0.53 - ETA: 38s - loss: 1.0620 - acc: 0.53 - ETA: 37s - loss: 1.0637 - acc: 0.53 - ETA: 37s - loss: 1.0588 - acc: 0.54 - ETA: 37s - loss: 1.0480 - acc: 0.54 - ETA: 36s - loss: 1.0558 - acc: 0.54 - ETA: 36s - loss: 1.0606 - acc: 0.54 - ETA: 35s - loss: 1.0590 - acc: 0.54 - ETA: 35s - loss: 1.0494 - acc: 0.55 - ETA: 35s - loss: 1.0427 - acc: 0.55 - ETA: 34s - loss: 1.0432 - acc: 0.55 - ETA: 34s - loss: 1.0409 - acc: 0.55 - ETA: 34s - loss: 1.0352 - acc: 0.55 - ETA: 33s - loss: 1.0293 - acc: 0.56 - ETA: 33s - loss: 1.0253 - acc: 0.56 - ETA: 32s - loss: 1.0205 - acc: 0.56 - ETA: 32s - loss: 1.0193 - acc: 0.56 - ETA: 32s - loss: 1.0196 - acc: 0.56 - ETA: 31s - loss: 1.0204 - acc: 0.56 - ETA: 31s - loss: 1.0215 - acc: 0.56 - ETA: 31s - loss: 1.0235 - acc: 0.56 - ETA: 30s - loss: 1.0197 - acc: 0.57 - ETA: 30s - loss: 1.0271 - acc: 0.56 - ETA: 29s - loss: 1.0241 - acc: 0.56 - ETA: 29s - loss: 1.0184 - acc: 0.57 - ETA: 29s - loss: 1.0158 - acc: 0.57 - ETA: 28s - loss: 1.0187 - acc: 0.57 - ETA: 28s - loss: 1.0155 - acc: 0.57 - ETA: 27s - loss: 1.0125 - acc: 0.57 - ETA: 27s - loss: 1.0108 - acc: 0.57 - ETA: 27s - loss: 1.0115 - acc: 0.57 - ETA: 26s - loss: 1.0094 - acc: 0.57 - ETA: 26s - loss: 1.0137 - acc: 0.57 - ETA: 26s - loss: 1.0124 - acc: 0.57 - ETA: 25s - loss: 1.0088 - acc: 0.57 - ETA: 25s - loss: 1.0058 - acc: 0.58 - ETA: 24s - loss: 1.0042 - acc: 0.58 - ETA: 24s - loss: 1.0037 - acc: 0.58 - ETA: 24s - loss: 1.0040 - acc: 0.57 - ETA: 23s - loss: 1.0028 - acc: 0.58 - ETA: 23s - loss: 1.0057 - acc: 0.57 - ETA: 22s - loss: 1.0052 - acc: 0.57 - ETA: 22s - loss: 1.0059 - acc: 0.57 - ETA: 22s - loss: 1.0048 - acc: 0.57 - ETA: 21s - loss: 1.0013 - acc: 0.58 - ETA: 21s - loss: 0.9983 - acc: 0.58 - ETA: 21s - loss: 0.9952 - acc: 0.58 - ETA: 20s - loss: 0.9964 - acc: 0.58 - ETA: 20s - loss: 0.9955 - acc: 0.58 - ETA: 19s - loss: 0.9957 - acc: 0.58 - ETA: 19s - loss: 0.9943 - acc: 0.58 - ETA: 19s - loss: 0.9978 - acc: 0.58 - ETA: 18s - loss: 0.9967 - acc: 0.58 - ETA: 18s - loss: 0.9957 - acc: 0.58 - ETA: 17s - loss: 0.9932 - acc: 0.58 - ETA: 17s - loss: 0.9924 - acc: 0.58 - ETA: 17s - loss: 0.9944 - acc: 0.58 - ETA: 16s - loss: 0.9931 - acc: 0.58 - ETA: 16s - loss: 0.9913 - acc: 0.58 - ETA: 15s - loss: 0.9879 - acc: 0.58 - ETA: 15s - loss: 0.9880 - acc: 0.58 - ETA: 15s - loss: 0.9867 - acc: 0.58 - ETA: 14s - loss: 0.9836 - acc: 0.59 - ETA: 14s - loss: 0.9837 - acc: 0.59 - ETA: 14s - loss: 0.9806 - acc: 0.59 - ETA: 13s - loss: 0.9808 - acc: 0.59 - ETA: 13s - loss: 0.9784 - acc: 0.59 - ETA: 12s - loss: 0.9785 - acc: 0.59 - ETA: 12s - loss: 0.9804 - acc: 0.59 - ETA: 12s - loss: 0.9807 - acc: 0.59 - ETA: 11s - loss: 0.9802 - acc: 0.59 - ETA: 11s - loss: 0.9790 - acc: 0.59 - ETA: 10s - loss: 0.9776 - acc: 0.59 - ETA: 10s - loss: 0.9772 - acc: 0.59 - ETA: 10s - loss: 0.9756 - acc: 0.59 - ETA: 9s - loss: 0.9708 - acc: 0.5972 - ETA: 9s - loss: 0.9691 - acc: 0.597 - ETA: 9s - loss: 0.9697 - acc: 0.597 - ETA: 8s - loss: 0.9712 - acc: 0.597 - ETA: 8s - loss: 0.9688 - acc: 0.599 - ETA: 7s - loss: 0.9655 - acc: 0.600 - ETA: 7s - loss: 0.9675 - acc: 0.600 - ETA: 7s - loss: 0.9682 - acc: 0.599 - ETA: 6s - loss: 0.9674 - acc: 0.600 - ETA: 6s - loss: 0.9652 - acc: 0.600 - ETA: 5s - loss: 0.9632 - acc: 0.601 - ETA: 5s - loss: 0.9600 - acc: 0.602 - ETA: 5s - loss: 0.9603 - acc: 0.601 - ETA: 4s - loss: 0.9581 - acc: 0.602 - ETA: 4s - loss: 0.9569 - acc: 0.602 - ETA: 3s - loss: 0.9550 - acc: 0.602 - ETA: 3s - loss: 0.9551 - acc: 0.602 - ETA: 3s - loss: 0.9515 - acc: 0.604 - ETA: 2s - loss: 0.9492 - acc: 0.606 - ETA: 2s - loss: 0.9512 - acc: 0.605 - ETA: 2s - loss: 0.9524 - acc: 0.606 - ETA: 1s - loss: 0.9535 - acc: 0.605 - ETA: 1s - loss: 0.9545 - acc: 0.605 - ETA: 0s - loss: 0.9546 - acc: 0.604 - ETA: 0s - loss: 0.9537 - acc: 0.605 - ETA: 0s - loss: 0.9532 - acc: 0.605 - 58s 22ms/step - loss: 0.9527 - acc: 0.6059 - val_loss: 1.5475 - val_acc: 0.4009\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.29984 to 1.54749, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624/2624 [==============================] - ETA: 50s - loss: 0.9754 - acc: 0.60 - ETA: 49s - loss: 0.8149 - acc: 0.67 - ETA: 48s - loss: 0.7844 - acc: 0.71 - ETA: 49s - loss: 0.7053 - acc: 0.77 - ETA: 48s - loss: 0.7137 - acc: 0.77 - ETA: 48s - loss: 0.7108 - acc: 0.75 - ETA: 47s - loss: 0.7103 - acc: 0.75 - ETA: 47s - loss: 0.7025 - acc: 0.75 - ETA: 47s - loss: 0.7019 - acc: 0.75 - ETA: 46s - loss: 0.6997 - acc: 0.76 - ETA: 46s - loss: 0.7271 - acc: 0.74 - ETA: 45s - loss: 0.7170 - acc: 0.74 - ETA: 45s - loss: 0.7025 - acc: 0.74 - ETA: 45s - loss: 0.7108 - acc: 0.73 - ETA: 44s - loss: 0.6995 - acc: 0.74 - ETA: 44s - loss: 0.7145 - acc: 0.73 - ETA: 43s - loss: 0.7119 - acc: 0.73 - ETA: 43s - loss: 0.6952 - acc: 0.73 - ETA: 43s - loss: 0.6878 - acc: 0.73 - ETA: 42s - loss: 0.6884 - acc: 0.73 - ETA: 42s - loss: 0.6931 - acc: 0.72 - ETA: 41s - loss: 0.6945 - acc: 0.73 - ETA: 41s - loss: 0.7022 - acc: 0.72 - ETA: 41s - loss: 0.6998 - acc: 0.72 - ETA: 40s - loss: 0.6994 - acc: 0.72 - ETA: 40s - loss: 0.6920 - acc: 0.72 - ETA: 40s - loss: 0.6954 - acc: 0.72 - ETA: 39s - loss: 0.6892 - acc: 0.73 - ETA: 39s - loss: 0.6794 - acc: 0.73 - ETA: 39s - loss: 0.6733 - acc: 0.74 - ETA: 38s - loss: 0.6741 - acc: 0.73 - ETA: 38s - loss: 0.6703 - acc: 0.73 - ETA: 37s - loss: 0.6745 - acc: 0.73 - ETA: 37s - loss: 0.6815 - acc: 0.73 - ETA: 37s - loss: 0.6749 - acc: 0.73 - ETA: 36s - loss: 0.6648 - acc: 0.74 - ETA: 36s - loss: 0.6609 - acc: 0.74 - ETA: 35s - loss: 0.6586 - acc: 0.74 - ETA: 35s - loss: 0.6644 - acc: 0.74 - ETA: 35s - loss: 0.6724 - acc: 0.73 - ETA: 34s - loss: 0.6647 - acc: 0.74 - ETA: 34s - loss: 0.6678 - acc: 0.73 - ETA: 34s - loss: 0.6842 - acc: 0.73 - ETA: 33s - loss: 0.6940 - acc: 0.73 - ETA: 33s - loss: 0.6934 - acc: 0.73 - ETA: 33s - loss: 0.6976 - acc: 0.72 - ETA: 32s - loss: 0.6967 - acc: 0.72 - ETA: 32s - loss: 0.6941 - acc: 0.72 - ETA: 31s - loss: 0.6958 - acc: 0.72 - ETA: 31s - loss: 0.6911 - acc: 0.72 - ETA: 31s - loss: 0.7009 - acc: 0.72 - ETA: 30s - loss: 0.7049 - acc: 0.72 - ETA: 30s - loss: 0.7062 - acc: 0.72 - ETA: 29s - loss: 0.7023 - acc: 0.72 - ETA: 29s - loss: 0.7077 - acc: 0.72 - ETA: 29s - loss: 0.7056 - acc: 0.72 - ETA: 28s - loss: 0.7050 - acc: 0.72 - ETA: 28s - loss: 0.7050 - acc: 0.72 - ETA: 28s - loss: 0.7069 - acc: 0.72 - ETA: 27s - loss: 0.7048 - acc: 0.72 - ETA: 27s - loss: 0.7052 - acc: 0.72 - ETA: 26s - loss: 0.7018 - acc: 0.72 - ETA: 26s - loss: 0.6999 - acc: 0.72 - ETA: 26s - loss: 0.6992 - acc: 0.72 - ETA: 25s - loss: 0.6987 - acc: 0.72 - ETA: 25s - loss: 0.6961 - acc: 0.72 - ETA: 24s - loss: 0.6944 - acc: 0.72 - ETA: 24s - loss: 0.6945 - acc: 0.72 - ETA: 24s - loss: 0.6909 - acc: 0.72 - ETA: 23s - loss: 0.6917 - acc: 0.72 - ETA: 23s - loss: 0.6899 - acc: 0.72 - ETA: 22s - loss: 0.6882 - acc: 0.73 - ETA: 22s - loss: 0.6856 - acc: 0.73 - ETA: 22s - loss: 0.6835 - acc: 0.73 - ETA: 21s - loss: 0.6834 - acc: 0.73 - ETA: 21s - loss: 0.6849 - acc: 0.73 - ETA: 21s - loss: 0.6872 - acc: 0.73 - ETA: 20s - loss: 0.6838 - acc: 0.73 - ETA: 20s - loss: 0.6805 - acc: 0.73 - ETA: 19s - loss: 0.6760 - acc: 0.73 - ETA: 19s - loss: 0.6771 - acc: 0.73 - ETA: 19s - loss: 0.6744 - acc: 0.73 - ETA: 18s - loss: 0.6705 - acc: 0.73 - ETA: 18s - loss: 0.6686 - acc: 0.74 - ETA: 17s - loss: 0.6692 - acc: 0.73 - ETA: 17s - loss: 0.6668 - acc: 0.74 - ETA: 17s - loss: 0.6663 - acc: 0.73 - ETA: 16s - loss: 0.6656 - acc: 0.73 - ETA: 16s - loss: 0.6626 - acc: 0.74 - ETA: 15s - loss: 0.6604 - acc: 0.74 - ETA: 15s - loss: 0.6615 - acc: 0.74 - ETA: 15s - loss: 0.6591 - acc: 0.74 - ETA: 14s - loss: 0.6606 - acc: 0.74 - ETA: 14s - loss: 0.6596 - acc: 0.74 - ETA: 14s - loss: 0.6558 - acc: 0.74 - ETA: 13s - loss: 0.6553 - acc: 0.74 - ETA: 13s - loss: 0.6578 - acc: 0.74 - ETA: 12s - loss: 0.6561 - acc: 0.74 - ETA: 12s - loss: 0.6551 - acc: 0.74 - ETA: 12s - loss: 0.6526 - acc: 0.74 - ETA: 11s - loss: 0.6536 - acc: 0.74 - ETA: 11s - loss: 0.6522 - acc: 0.74 - ETA: 10s - loss: 0.6530 - acc: 0.74 - ETA: 10s - loss: 0.6534 - acc: 0.74 - ETA: 10s - loss: 0.6513 - acc: 0.74 - ETA: 9s - loss: 0.6530 - acc: 0.7467 - ETA: 9s - loss: 0.6533 - acc: 0.746 - ETA: 9s - loss: 0.6538 - acc: 0.746 - ETA: 8s - loss: 0.6540 - acc: 0.747 - ETA: 8s - loss: 0.6543 - acc: 0.747 - ETA: 7s - loss: 0.6577 - acc: 0.745 - ETA: 7s - loss: 0.6587 - acc: 0.745 - ETA: 7s - loss: 0.6586 - acc: 0.744 - ETA: 6s - loss: 0.6587 - acc: 0.744 - ETA: 6s - loss: 0.6576 - acc: 0.745 - ETA: 5s - loss: 0.6560 - acc: 0.745 - ETA: 5s - loss: 0.6561 - acc: 0.744 - ETA: 5s - loss: 0.6530 - acc: 0.746 - ETA: 4s - loss: 0.6518 - acc: 0.746 - ETA: 4s - loss: 0.6488 - acc: 0.747 - ETA: 3s - loss: 0.6505 - acc: 0.745 - ETA: 3s - loss: 0.6487 - acc: 0.746 - ETA: 3s - loss: 0.6525 - acc: 0.744 - ETA: 2s - loss: 0.6522 - acc: 0.744 - ETA: 2s - loss: 0.6516 - acc: 0.744 - ETA: 2s - loss: 0.6516 - acc: 0.744 - ETA: 1s - loss: 0.6505 - acc: 0.744 - ETA: 1s - loss: 0.6490 - acc: 0.745 - ETA: 0s - loss: 0.6471 - acc: 0.746 - ETA: 0s - loss: 0.6466 - acc: 0.747 - ETA: 0s - loss: 0.6450 - acc: 0.748 - 57s 22ms/step - loss: 0.6442 - acc: 0.7485 - val_loss: 1.7222 - val_acc: 0.3742\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.54749\n",
      "Epoch 4/4\n",
      "2624/2624 [==============================] - ETA: 48s - loss: 0.7776 - acc: 0.60 - ETA: 48s - loss: 0.6838 - acc: 0.70 - ETA: 48s - loss: 0.6401 - acc: 0.70 - ETA: 49s - loss: 0.6108 - acc: 0.71 - ETA: 48s - loss: 0.6215 - acc: 0.71 - ETA: 48s - loss: 0.5881 - acc: 0.72 - ETA: 47s - loss: 0.5673 - acc: 0.74 - ETA: 47s - loss: 0.5635 - acc: 0.74 - ETA: 46s - loss: 0.5657 - acc: 0.75 - ETA: 46s - loss: 0.5402 - acc: 0.76 - ETA: 46s - loss: 0.5164 - acc: 0.77 - ETA: 45s - loss: 0.5284 - acc: 0.77 - ETA: 45s - loss: 0.5256 - acc: 0.76 - ETA: 45s - loss: 0.5175 - acc: 0.77 - ETA: 44s - loss: 0.4951 - acc: 0.78 - ETA: 44s - loss: 0.4841 - acc: 0.79 - ETA: 44s - loss: 0.4764 - acc: 0.80 - ETA: 43s - loss: 0.4691 - acc: 0.80 - ETA: 43s - loss: 0.4615 - acc: 0.80 - ETA: 43s - loss: 0.4576 - acc: 0.81 - ETA: 42s - loss: 0.4506 - acc: 0.81 - ETA: 42s - loss: 0.4468 - acc: 0.82 - ETA: 42s - loss: 0.4505 - acc: 0.82 - ETA: 41s - loss: 0.4627 - acc: 0.81 - ETA: 41s - loss: 0.4582 - acc: 0.82 - ETA: 40s - loss: 0.4584 - acc: 0.82 - ETA: 40s - loss: 0.4580 - acc: 0.82 - ETA: 40s - loss: 0.4615 - acc: 0.81 - ETA: 39s - loss: 0.4554 - acc: 0.82 - ETA: 39s - loss: 0.4677 - acc: 0.82 - ETA: 39s - loss: 0.4673 - acc: 0.82 - ETA: 38s - loss: 0.4608 - acc: 0.82 - ETA: 38s - loss: 0.4596 - acc: 0.82 - ETA: 37s - loss: 0.4540 - acc: 0.82 - ETA: 37s - loss: 0.4529 - acc: 0.82 - ETA: 37s - loss: 0.4543 - acc: 0.82 - ETA: 36s - loss: 0.4580 - acc: 0.82 - ETA: 36s - loss: 0.4536 - acc: 0.82 - ETA: 35s - loss: 0.4594 - acc: 0.82 - ETA: 35s - loss: 0.4636 - acc: 0.82 - ETA: 35s - loss: 0.4618 - acc: 0.82 - ETA: 34s - loss: 0.4637 - acc: 0.82 - ETA: 34s - loss: 0.4560 - acc: 0.82 - ETA: 33s - loss: 0.4521 - acc: 0.82 - ETA: 33s - loss: 0.4456 - acc: 0.83 - ETA: 33s - loss: 0.4423 - acc: 0.83 - ETA: 32s - loss: 0.4365 - acc: 0.83 - ETA: 32s - loss: 0.4302 - acc: 0.83 - ETA: 31s - loss: 0.4303 - acc: 0.83 - ETA: 31s - loss: 0.4282 - acc: 0.84 - ETA: 31s - loss: 0.4345 - acc: 0.83 - ETA: 30s - loss: 0.4328 - acc: 0.83 - ETA: 30s - loss: 0.4346 - acc: 0.83 - ETA: 30s - loss: 0.4320 - acc: 0.83 - ETA: 29s - loss: 0.4271 - acc: 0.84 - ETA: 29s - loss: 0.4258 - acc: 0.84 - ETA: 28s - loss: 0.4248 - acc: 0.84 - ETA: 28s - loss: 0.4246 - acc: 0.84 - ETA: 28s - loss: 0.4229 - acc: 0.84 - ETA: 27s - loss: 0.4230 - acc: 0.84 - ETA: 27s - loss: 0.4190 - acc: 0.84 - ETA: 26s - loss: 0.4156 - acc: 0.84 - ETA: 26s - loss: 0.4139 - acc: 0.84 - ETA: 26s - loss: 0.4161 - acc: 0.84 - ETA: 25s - loss: 0.4226 - acc: 0.84 - ETA: 25s - loss: 0.4223 - acc: 0.84 - ETA: 24s - loss: 0.4235 - acc: 0.84 - ETA: 24s - loss: 0.4219 - acc: 0.84 - ETA: 24s - loss: 0.4213 - acc: 0.84 - ETA: 23s - loss: 0.4180 - acc: 0.84 - ETA: 23s - loss: 0.4161 - acc: 0.85 - ETA: 23s - loss: 0.4131 - acc: 0.85 - ETA: 22s - loss: 0.4120 - acc: 0.85 - ETA: 22s - loss: 0.4200 - acc: 0.84 - ETA: 21s - loss: 0.4177 - acc: 0.84 - ETA: 21s - loss: 0.4271 - acc: 0.84 - ETA: 21s - loss: 0.4301 - acc: 0.84 - ETA: 20s - loss: 0.4288 - acc: 0.84 - ETA: 20s - loss: 0.4253 - acc: 0.84 - ETA: 19s - loss: 0.4244 - acc: 0.84 - ETA: 19s - loss: 0.4254 - acc: 0.84 - ETA: 19s - loss: 0.4267 - acc: 0.84 - ETA: 18s - loss: 0.4248 - acc: 0.84 - ETA: 18s - loss: 0.4239 - acc: 0.84 - ETA: 17s - loss: 0.4224 - acc: 0.85 - ETA: 17s - loss: 0.4186 - acc: 0.85 - ETA: 17s - loss: 0.4178 - acc: 0.85 - ETA: 16s - loss: 0.4166 - acc: 0.85 - ETA: 16s - loss: 0.4170 - acc: 0.85 - ETA: 16s - loss: 0.4160 - acc: 0.85 - ETA: 15s - loss: 0.4150 - acc: 0.85 - ETA: 15s - loss: 0.4176 - acc: 0.85 - ETA: 14s - loss: 0.4165 - acc: 0.85 - ETA: 14s - loss: 0.4151 - acc: 0.85 - ETA: 14s - loss: 0.4138 - acc: 0.85 - ETA: 13s - loss: 0.4152 - acc: 0.85 - ETA: 13s - loss: 0.4187 - acc: 0.84 - ETA: 12s - loss: 0.4219 - acc: 0.84 - ETA: 12s - loss: 0.4207 - acc: 0.84 - ETA: 12s - loss: 0.4192 - acc: 0.84 - ETA: 11s - loss: 0.4211 - acc: 0.84 - ETA: 11s - loss: 0.4240 - acc: 0.84 - ETA: 10s - loss: 0.4265 - acc: 0.84 - ETA: 10s - loss: 0.4283 - acc: 0.84 - ETA: 10s - loss: 0.4297 - acc: 0.84 - ETA: 9s - loss: 0.4292 - acc: 0.8458 - ETA: 9s - loss: 0.4269 - acc: 0.846 - ETA: 9s - loss: 0.4270 - acc: 0.846 - ETA: 8s - loss: 0.4250 - acc: 0.847 - ETA: 8s - loss: 0.4258 - acc: 0.848 - ETA: 7s - loss: 0.4250 - acc: 0.848 - ETA: 7s - loss: 0.4269 - acc: 0.848 - ETA: 7s - loss: 0.4275 - acc: 0.847 - ETA: 6s - loss: 0.4271 - acc: 0.847 - ETA: 6s - loss: 0.4250 - acc: 0.848 - ETA: 5s - loss: 0.4236 - acc: 0.848 - ETA: 5s - loss: 0.4241 - acc: 0.848 - ETA: 5s - loss: 0.4240 - acc: 0.848 - ETA: 4s - loss: 0.4237 - acc: 0.848 - ETA: 4s - loss: 0.4244 - acc: 0.847 - ETA: 3s - loss: 0.4239 - acc: 0.847 - ETA: 3s - loss: 0.4242 - acc: 0.847 - ETA: 3s - loss: 0.4226 - acc: 0.848 - ETA: 2s - loss: 0.4215 - acc: 0.848 - ETA: 2s - loss: 0.4207 - acc: 0.849 - ETA: 2s - loss: 0.4206 - acc: 0.849 - ETA: 1s - loss: 0.4229 - acc: 0.848 - ETA: 1s - loss: 0.4231 - acc: 0.848 - ETA: 0s - loss: 0.4258 - acc: 0.846 - ETA: 0s - loss: 0.4240 - acc: 0.847 - ETA: 0s - loss: 0.4224 - acc: 0.847 - 57s 22ms/step - loss: 0.4224 - acc: 0.8476 - val_loss: 1.4636 - val_acc: 0.4574\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.54749 to 1.46364, saving model to saved_models/weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21618edccc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 57.8000%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted diagnosis for each image in test set\n",
    "diagnosis_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(diagnosis_predictions)==np.argmax(test_targets, axis=1))/len(diagnosis_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step N: Create a CNN to Classify Diagnosis (using Transfer Learning)\n",
    "\n",
    "\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "\n",
    "ResNet50_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = ResNet50_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(4, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "\n",
    "#x = ResNet50_model.output\n",
    "#x = Conv2D(filters=75, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3))(x)\n",
    "#x = MaxPooling2D(pool_size=2)(x)\n",
    "#x = Conv2D(filters=100, kernel_size=1, padding='same', activation='relu')(x)\n",
    "#x = MaxPooling2D(pool_size=2)(x)\n",
    "#x = Conv2D(filters=125, kernel_size=1, padding='same', activation='relu')(x)    \n",
    "#x = MaxPooling2D(pool_size=2)(x)\n",
    "#x = Conv2D(filters=125, kernel_size=1, padding='same', activation='relu')(x)   \n",
    "#x = MaxPooling2D(pool_size=2)(x)\n",
    "#x = Conv2D(filters=125, kernel_size=1, padding='same', activation='relu')(x)   \n",
    "#x = MaxPooling2D(pool_size=2)(x)\n",
    "#x = Conv2D(filters=125, kernel_size=1, padding='same', activation='relu')(x)    \n",
    "#x = MaxPooling2D(pool_size=2)(x)    \n",
    "#x = Conv2D(filters=125, kernel_size=1, padding='same', activation='relu')(x)    \n",
    "#x = MaxPooling2D(pool_size=2)(x)\n",
    "#x = Dropout(0.3)(x)\n",
    "##x = Flatten()(x)\n",
    "#predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# this is the model we will train\n",
    "ResNet50_transfer_model = Model(inputs=ResNet50_model.input, outputs=predictions)\n",
    "\n",
    "### first: train only the top layers (which were randomly initialized)\n",
    "### i.e. freeze all convolutional InceptionV3 layers\n",
    "##for layer in ResNet50_model.layers:\n",
    "##    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "ResNet50_transfer_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2624 samples, validate on 938 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[20,192,52,52] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_8/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_5/RMSprop/gradients/conv2d_8/convolution_grad/Conv2DBackpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_53/Relu, conv2d_8/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_5/RMSprop/gradients/batch_normalization_57/cond/batchnorm/add_1_grad/Shape/_14165 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6525_...grad/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'conv2d_8/convolution', defined at:\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-3275ce16cef3>\", line 5, in <module>\n    ResNet50_model = InceptionV3(weights='imagenet', include_top=False)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\applications\\inception_v3.py\", line 181, in InceptionV3\n    x = conv2d_bn(x, 192, 3, 3, padding='valid')\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\applications\\inception_v3.py\", line 81, in conv2d_bn\n    name=conv_name)(x)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3341, in conv2d\n    data_format=tf_data_format)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 780, in convolution\n    return op(input, filter)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 868, in __call__\n    return self.conv_op(inp, filter)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 520, in __call__\n    return self.call(inp, filter)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 204, in __call__\n    name=self.name)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1042, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[20,192,52,52] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_8/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_5/RMSprop/gradients/conv2d_8/convolution_grad/Conv2DBackpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_53/Relu, conv2d_8/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_5/RMSprop/gradients/batch_normalization_57/cond/batchnorm/add_1_grad/Shape/_14165 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6525_...grad/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,192,52,52] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_8/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_5/RMSprop/gradients/conv2d_8/convolution_grad/Conv2DBackpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_53/Relu, conv2d_8/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_5/RMSprop/gradients/batch_normalization_57/cond/batchnorm/add_1_grad/Shape/_14165 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6525_...grad/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d44217ba4bc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m ResNet50_transfer_model.fit(train_tensors, train_targets, \n\u001b[0;32m     10\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                             epochs=ResNet50_epochs, batch_size=20, callbacks=[ResNet50_checkpointer], verbose=1)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# at this point, the top layers are well trained and we can start fine-tuning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,192,52,52] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_8/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_5/RMSprop/gradients/conv2d_8/convolution_grad/Conv2DBackpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_53/Relu, conv2d_8/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_5/RMSprop/gradients/batch_normalization_57/cond/batchnorm/add_1_grad/Shape/_14165 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6525_...grad/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'conv2d_8/convolution', defined at:\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-3275ce16cef3>\", line 5, in <module>\n    ResNet50_model = InceptionV3(weights='imagenet', include_top=False)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\applications\\inception_v3.py\", line 181, in InceptionV3\n    x = conv2d_bn(x, 192, 3, 3, padding='valid')\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\applications\\inception_v3.py\", line 81, in conv2d_bn\n    name=conv_name)(x)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3341, in conv2d\n    data_format=tf_data_format)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 780, in convolution\n    return op(input, filter)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 868, in __call__\n    return self.conv_op(inp, filter)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 520, in __call__\n    return self.call(inp, filter)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 204, in __call__\n    name=self.name)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1042, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[20,192,52,52] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_8/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_5/RMSprop/gradients/conv2d_8/convolution_grad/Conv2DBackpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_53/Relu, conv2d_8/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_5/RMSprop/gradients/batch_normalization_57/cond/batchnorm/add_1_grad/Shape/_14165 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6525_...grad/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "ResNet50_epochs = 4\n",
    "\n",
    "ResNet50_checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.ResNet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "ResNet50_transfer_model.fit(train_tensors, train_targets, \n",
    "                            validation_data=(valid_tensors, valid_targets),\n",
    "                            epochs=ResNet50_epochs, batch_size=20, callbacks=[ResNet50_checkpointer], verbose=1)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(ResNet50_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in ResNet50_transfer_model.layers[:174]:\n",
    "   layer.trainable = False\n",
    "for layer in ResNet50_transfer_model.layers[174:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "ResNet50_transfer_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "ResNet50_transfer_model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=ResNet50_epochs, batch_size=20, callbacks=[ResNet50_checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights that yielded the best validation accuracy# load t \n",
    "model.load_weights('saved_models/weights.best.ResNet50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "#return ResNet50(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n",
    "\n",
    "\n",
    "def transfer_learning_model_make_prediction(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = preprocess_input(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return np.argmax(predicted_vector)\n",
    "\n",
    "\n",
    "file_to_test = test_files[800]\n",
    "\n",
    "print(oct_names)\n",
    "print(file_to_test)\n",
    "print(transfer_learning_model_make_prediction(file_to_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(test_targets)\n",
    "\n",
    "# get index of predicted dog breed for each image in test set\n",
    "transfer_learning_model_predictions = [transfer_learning_model_make_prediction(file) for file in test_files]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(transfer_learning_model_predictions)==np.argmax(test_targets, axis=1))/len(transfer_learning_model_predictions)\n",
    "\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
